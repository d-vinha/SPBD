{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-vinha/SPBD/blob/main/lab5/SPBD_Labs_spark2_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtsPM1Z4HH7M"
      },
      "source": [
        "# Python Spark Dataframes Exercises\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Pyspark\n",
        "!pip install --quiet pyspark"
      ],
      "metadata": {
        "id": "BuFS4wO2B1vr",
        "outputId": "8b7dd517-95a7-4e31-cccd-189af521ae02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download \"Os Maias\"\n",
        "!wget -q -O os_maias.txt https://www.dropbox.com/s/n24v0z7y79np319/os_maias.txt?dl=0"
      ],
      "metadata": {
        "id": "GEn0_HxQHDlx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Sorted Word Frequency\n",
        "\n",
        "1.1) Create a [Spark Dataframes](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html) program that counts the number of occurrences of each word in \"Os Maias\" novel, sorting them by frequency (the words with higher occurrence first).\n"
      ],
      "metadata": {
        "id": "f7oXYnylGyko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try :\n",
        "  # Read the text file and filter out empty lines\n",
        "  lines = sc.textFile('os_maias.txt') \\\n",
        "  .filter( lambda line : len(line) > 1 )\n",
        "\n",
        "  # Create a DataFrame with 'line' and 'listOfWords' columns\n",
        "  structured_lines = lines.map( lambda line : Row( line = line, listOfWords = line.split(' ') ) )\n",
        "  wordsOfLine = spark.createDataFrame( structured_lines )\n",
        "\n",
        "  # Explode the 'listOfWords' array into separate rows\n",
        "  # The explode function is used to transform the array of words in the\n",
        "  # listOfWords column into separate rows. Each word becomes a new row, and we\n",
        "  # alias this as 'words'.\n",
        "    # We then group the DataFrame by the 'words' column (which now contains\n",
        "    # individual words) and count the occurrences of each word.\n",
        "    # Order by count in descending order\n",
        "  x = wordsOfLine.select(explode(\"listOfWords\").alias('words')) \\\n",
        "      .groupBy('words').count() \\\n",
        "      .orderBy('count', ascending=False)\n",
        "\n",
        "  # Show the top 5 words by count\n",
        "  x.show(5)\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "  sc.stop()"
      ],
      "metadata": {
        "id": "qMFaHhpWHxkB",
        "outputId": "fd8cc03c-d4dd-47e0-8eef-91e3ad93b60f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|words|count|\n",
            "+-----+-----+\n",
            "|   de| 8308|\n",
            "|    a| 6720|\n",
            "|    o| 6602|\n",
            "|  que| 4846|\n",
            "|    e| 4441|\n",
            "+-----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2) Create a Spark Dataframes program that computes the top 10 most used words in \"Os Maias\" novel."
      ],
      "metadata": {
        "id": "UkI4QSo8Ua35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try :\n",
        "  lines = sc.textFile('os_maias.txt') \\\n",
        "  .filter( lambda line : len(line) > 1 )\n",
        "\n",
        "  structured_lines = lines.map( lambda line : Row( line = line, listOfWords = line.split(' ') ) )\n",
        "\n",
        "  wordsOfLine = spark.createDataFrame( structured_lines )\n",
        "\n",
        "  x = wordsOfLine.select(explode(\"listOfWords\").alias('words')) \\\n",
        "      .groupBy('words').count() \\\n",
        "      .orderBy('count', ascending=False) \\\n",
        "      .limit(10)\n",
        "      # .limit(10) is used at the end of the DataFrame operations. It limits the\n",
        "      # output to the first 10 rows based on the ordering (in this case, based\n",
        "      # on word count in descending order).\n",
        "\n",
        "\n",
        "  x.show()\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "  sc.stop()"
      ],
      "metadata": {
        "id": "jocH9TZyUbMF",
        "outputId": "c5c1456f-6330-40e8-bbea-74f952afea95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|words|count|\n",
            "+-----+-----+\n",
            "|   de| 8308|\n",
            "|    a| 6720|\n",
            "|    o| 6602|\n",
            "|  que| 4846|\n",
            "|    e| 4441|\n",
            "|    -| 3535|\n",
            "|   um| 3004|\n",
            "|  com| 2792|\n",
            "|   do| 2564|\n",
            "|   da| 2200|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Weblog Analysis\n",
        "\n",
        "Consider a set of log files captured during a DDOS (*Distributed Denial of Service*) attack, containing information for the web accesses performed during the attack to the server.\n",
        "\n",
        "The log files contain text lines as shown below, with TAB as the separator:\n",
        "\n",
        "date |IP_source | status_code | operation | URL | execution time |\n",
        "-|-|-|-|-|-\n",
        "timestamp  | string | int | string | string| float |\n",
        "2016-12-06T08:58:35.318+0000|37.139.9.11|404|GET|/codemove/TTCENCUFMH3C|0.026"
      ],
      "metadata": {
        "id": "rsJZWYlHZDJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download the dataset\n",
        "!wget -q -O web.log https://www.dropbox.com/s/0r8902uj9yum7dg/web.log?dl=0\n",
        "!head -1 web.log"
      ],
      "metadata": {
        "id": "WCWKj68qCOdA",
        "outputId": "5faf90cc-70cc-4c03-b01c-4df6574358f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2016-12-06T08:58:35.318+0000 37.139.9.11 404 GET /codemove/TTCENCUFMH3C 0.026  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1. Count the number of unique IP addresses involved in the DDOS attack.\n"
      ],
      "metadata": {
        "id": "N1-ojIAqCftf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "\t\t\t\t\t\t.appName('weblog').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "try :\n",
        "    lines = sc.textFile('web.log')\n",
        "    logRows = lines.filter( lambda line : len(line) > 0 ) \\\n",
        "                   .map( lambda line : line.split(' ') ) \\\n",
        "                   .map( lambda l : Row( date = l[0], \\\n",
        "\t\t\t\t    \t\t            ipSource = l[1], retValue = l[2], \\\n",
        "                            op = l[3], url = l[4], time = float(l[5])))\n",
        "\n",
        "    logRowsDF = spark.createDataFrame( logRows )\n",
        "    countIps = logRowsDF.select('ipSource').distinct().count()\n",
        "\n",
        "    print(countIps)\n",
        "    sc.stop()\n",
        "except Exception as err:\n",
        "    print(err)\n",
        "    sc.stop()"
      ],
      "metadata": {
        "id": "Y7XoyNETChb-",
        "outputId": "84005de7-eee8-4962-e373-738cf777722a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2. For each interval of 10 seconds, provide the following information: [number of requests, average execution time, maximum time, minimum time]"
      ],
      "metadata": {
        "id": "ZJ5TzPdACgQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "\t\t\t\t\t\t.appName('weblog').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "try :\n",
        "    lines = sc.textFile('web.log')\n",
        "    logRows = lines.filter( lambda line : len(line) > 0 ) \\\n",
        "                   .map( lambda line : line.split(' ') ) \\\n",
        "                   .map( lambda l : Row( date = l[0], \\\n",
        "\t\t\t\t    \t\t            ipSource = l[1], retValue = l[2], \\\n",
        "                            op = l[3], url = l[4], time = float(l[5])))\n",
        "\n",
        "\n",
        "    interval = udf(lambda timestamp: timestamp[0:18], StringType())\n",
        "\n",
        "\n",
        "    logRowsDF = spark.createDataFrame( logRows )\n",
        "    intervals = logRowsDF.select(interval('date').alias(\"interval\"), \"time\")\n",
        "    x = intervals.groupBy('interval').agg( count('*').alias('count'), avg('time'), min('time'), max('time')) \\\n",
        "        .orderBy('interval')\n",
        "\n",
        "    x.show(10)\n",
        "    sc.stop()\n",
        "except Exception as err:\n",
        "    print(err)\n",
        "    sc.stop()"
      ],
      "metadata": {
        "id": "M8UVCwcdCwTG",
        "outputId": "49137df2-8c24-4cb1-a846-cf0b91a84798",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----+------------------+---------+---------+\n",
            "|          interval|count|         avg(time)|min(time)|max(time)|\n",
            "+------------------+-----+------------------+---------+---------+\n",
            "|2016-12-06T08:58:3|  483|7.5934244306418215|    0.013|   46.849|\n",
            "|2016-12-06T08:58:4| 2611|30.159845653006503|    0.014|   69.654|\n",
            "|2016-12-06T08:58:5| 5500| 38.52511163636371|    0.017|   80.846|\n",
            "|2016-12-06T08:59:0| 6914| 38.53438212322824|    0.018|   81.659|\n",
            "|2016-12-06T08:59:1| 6271| 32.96384978472328|    0.017|   83.993|\n",
            "|2016-12-06T08:59:2| 5434| 17.29333143172616|    0.051|   77.967|\n",
            "|2016-12-06T08:59:3| 8015|11.210152214597631|    0.056|   67.441|\n",
            "|2016-12-06T08:59:4| 7947| 7.761815779539431|    0.914|   65.706|\n",
            "|2016-12-06T08:59:5| 5983| 3.821664382416849|    0.678|    54.29|\n",
            "|2016-12-06T09:00:0| 6882| 8.649971519907023|    0.017|   45.314|\n",
            "+------------------+-----+------------------+---------+---------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, FloatType, IntegerType\n",
        "from pyspark.sql.functions import window, col, avg, max, min, count\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "\t\t\t\t\t\t.appName('weblog').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "try :\n",
        "    lines = sc.textFile('web.log')\n",
        "    logRows = lines.filter( lambda line : len(line) > 0 ) \\\n",
        "                   .map( lambda line : line.split(' ') )\n",
        "except Exception as err:\n",
        "    print(err)\n",
        "    sc.stop()\n",
        "\n",
        "# Define the schema for the data\n",
        "schema = StructType([\n",
        "    StructField(\"date\", StringType(), True),\n",
        "    StructField(\"IP_source\", StringType(), True),\n",
        "    StructField(\"status_code\", IntegerType(), True),\n",
        "    StructField(\"operation\", StringType(), True),\n",
        "    StructField(\"URL\", StringType(), True),\n",
        "    StructField(\"execution_time\", FloatType(), True)\n",
        "])\n",
        "\n",
        "interval = udf(lambda timestamp: timestamp[0:18], StringType())\n",
        "\n",
        "# Create a DataFrame from the sample data\n",
        "weblog_df = spark.createDataFrame(logRows, schema=schema)\n",
        "\n",
        "# Group data into 10-second intervals and calculate required statistics\n",
        "result_df = (\n",
        "    weblog_df.withColumn(\"interval\", window(\"date\", \"10 seconds\"))\n",
        "      .groupBy(interval('date'))\n",
        "      .agg(\n",
        "          count(\"operation\").alias(\"number_of_requests\"),\n",
        "          avg(\"execution_time\").alias(\"average_execution_time\"),\n",
        "          max(\"execution_time\").alias(\"maximum_time\"),\n",
        "          min(\"execution_time\").alias(\"minimum_time\")\n",
        "      )\n",
        "      .orderBy(\"interval\")\n",
        ")\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "FrQZ2gtg9qq0",
        "outputId": "221df319-98e9-47b1-e89e-b7da52055d6e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-e3fe269731d6>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m           \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"execution_time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"minimum_time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       )\n\u001b[0;32m---> 41\u001b[0;31m       \u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"interval\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36msort\u001b[0;34m(self, *cols, **kwargs)\u001b[0m\n\u001b[1;32m   2734\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2735\u001b[0m         \"\"\"\n\u001b[0;32m-> 2736\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2737\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `interval` cannot be resolved. Did you mean one of the following? [`minimum_time`, `maximum_time`, `<lambda>(date)`, `number_of_requests`, `average_execution_time`].;\n'Sort ['interval ASC NULLS FIRST], true\n+- Aggregate [<lambda>(date#662)#684], [<lambda>(date#662)#684 AS <lambda>(date)#700, count(operation#665) AS number_of_requests#693L, avg(execution_time#667) AS average_execution_time#695, max(execution_time#667) AS maximum_time#697, min(execution_time#667) AS minimum_time#699]\n   +- Project [date#662, IP_source#663, status_code#664, operation#665, URL#666, execution_time#667, window#676 AS interval#675]\n      +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(cast(date#662 as timestamp), TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(cast(date#662 as timestamp), TimestampType, LongType) - 0) % 10000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(cast(date#662 as timestamp), TimestampType, LongType) - 0) % 10000000) + 10000000) ELSE ((precisetimestampconversion(cast(date#662 as timestamp), TimestampType, LongType) - 0) % 10000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(cast(date#662 as timestamp), TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(cast(date#662 as timestamp), TimestampType, LongType) - 0) % 10000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(cast(date#662 as timestamp), TimestampType, LongType) - 0) % 10000000) + 10000000) ELSE ((precisetimestampconversion(cast(date#662 as timestamp), TimestampType, LongType) - 0) % 10000000) END) - 0) + 10000000), LongType, TimestampType))) AS window#676, date#662, IP_source#663, status_code#664, operation#665, URL#666, execution_time#667]\n         +- Filter isnotnull(cast(date#662 as timestamp))\n            +- LogicalRDD [date#662, IP_source#663, status_code#664, operation#665, URL#666, execution_time#667], false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3. Create an inverted index that, for each interval of 10 seconds, has a list of (unique) IPs executing accesses (to each URL)."
      ],
      "metadata": {
        "id": "jUHmctaICgtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "\t\t\t\t\t\t.appName('weblog').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "try :\n",
        "    lines = sc.textFile('web.log')\n",
        "    logRows = lines.filter( lambda line : len(line) > 0 ) \\\n",
        "                   .map( lambda line : line.split(' ') ) \\\n",
        "                   .map( lambda l : Row( date = l[0], \\\n",
        "\t\t\t\t    \t\t            ipSource = l[1], retValue = l[2], \\\n",
        "                            op = l[3], url = l[4], time = float(l[5])))\n",
        "\n",
        "\n",
        "    interval = udf(lambda timestamp: timestamp[0:18], StringType())\n",
        "\n",
        "    countIps = udf( lambda l : len(l))\n",
        "\n",
        "    logRowsDF = spark.createDataFrame( logRows )\n",
        "    intervals = logRowsDF.select(interval('date').alias(\"interval\"), 'ipSource', \"url\")\n",
        "\n",
        "    stats = intervals.groupBy('interval', 'url').agg( collect_set('ipSource').alias('ips')) \\\n",
        "    .orderBy('interval', 'url', ascending=False)\n",
        "\n",
        "    #stats = intervals.groupBy('interval', 'url').agg( collect_set('ipSource').alias('ips')) \\\n",
        "    #.select( \"*\", countIps('ips').alias('#ips')).orderBy('interval', 'url', '#ips', ascending=False)\n",
        "\n",
        "    stats.show(10)\n",
        "\n",
        "    sc.stop()\n",
        "except Exception as err:\n",
        "    print(err)\n",
        "    sc.stop()"
      ],
      "metadata": {
        "id": "RpXghha0C0jC",
        "outputId": "aa6d08cc-13a8-4ac0-f03d-1d9d970432f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+--------------------+--------------------+\n",
            "|          interval|                 url|                 ips|\n",
            "+------------------+--------------------+--------------------+\n",
            "|2016-12-06T10:03:2|/codemove/9JVQI8T...|[106.37.189.69, 1...|\n",
            "|2016-12-06T10:03:2|/codemove/79SC2H8...|[106.37.189.69, 1...|\n",
            "|2016-12-06T10:03:1|/codemove/NCZX4FB...|[106.37.189.69, 2...|\n",
            "|2016-12-06T10:03:1|/codemove/9JVQI8T...|[106.37.189.69, 2...|\n",
            "|2016-12-06T10:03:1|/codemove/79SC2H8...|[106.37.189.69, 2...|\n",
            "|2016-12-06T10:03:0|/codemove/NCZX4FB...|[106.37.189.69, 1...|\n",
            "|2016-12-06T10:03:0|/codemove/9JVQI8T...|[106.37.189.69, 1...|\n",
            "|2016-12-06T10:03:0|/codemove/79SC2H8...|[106.37.189.69, 1...|\n",
            "|2016-12-06T10:02:5|/codemove/9JVQI8T...|[106.37.189.69, 1...|\n",
            "|2016-12-06T10:02:5|/codemove/79SC2H8...|[106.37.189.69, 2...|\n",
            "+------------------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3. Create an inverted index that, for each interval of 15 seconds, has a list of (unique) IPs executing accesses (to each URL)."
      ],
      "metadata": {
        "id": "tpLWhBuRhpE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "from dateutil.parser import parse\n",
        "\n",
        "origin = parse('2016-12-06T08:58:35.318+0000').timestamp()\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "\t\t\t\t\t\t.appName('weblog').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "try :\n",
        "    lines = sc.textFile('web.log')\n",
        "    logRows = lines.filter( lambda line : len(line) > 0 ) \\\n",
        "                   .map( lambda line : line.split(' ') ) \\\n",
        "                   .map( lambda l : Row( date = l[0], \\\n",
        "\t\t\t\t    \t\t            ipSource = l[1], retValue = l[2], \\\n",
        "                            op = l[3], url = l[4], time = float(l[5])))\n",
        "\n",
        "    logRowsDF = spark.createDataFrame( logRows )\n",
        "\n",
        "    # use window() to define the interval\n",
        "    intervals = logRowsDF.select(col('date'), 'ipSource', \"url\") \\\n",
        "        .select('*', window(\"date\", \"15 seconds\").alias('interval'))\n",
        "\n",
        "    stats = intervals.groupBy('interval', 'url').agg( collect_set('ipSource').alias('ips')) \\\n",
        "    .select( \"*\", countIps('ips').alias('#ips')).orderBy('#ips', ascending=False)\n",
        "\n",
        "    stats.show(10)\n",
        "\n",
        "    sc.stop()\n",
        "except Exception as err:\n",
        "    print(err)\n",
        "    sc.stop()"
      ],
      "metadata": {
        "id": "_uRu80x9mE0_",
        "outputId": "10b62726-c9d9-4dc5-dc25-a627ad547e0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+----+\n",
            "|            interval|                 url|                 ips|#ips|\n",
            "+--------------------+--------------------+--------------------+----+\n",
            "|{2016-12-06 09:01...|/codemove/JUANR8S...|[95.128.43.164, 1...|   8|\n",
            "|{2016-12-06 09:02...|/codemove/JUANR8S...|[185.69.154.59, 5...|   5|\n",
            "|{2016-12-06 09:02...|/codemove/C11AWNK...|[185.69.154.59, 2...|   5|\n",
            "|{2016-12-06 09:01...|/codemove/JUANR8S...|[163.172.67.180, ...|   4|\n",
            "|{2016-12-06 10:02...|/codemove/79SC2H8...|[106.37.189.69, 2...|   3|\n",
            "|{2016-12-06 10:01...|/codemove/2J6KRPS...|[106.37.189.69, 2...|   3|\n",
            "|{2016-12-06 10:01...|/codemove/2J6KRPS...|[106.37.189.69, 2...|   3|\n",
            "|{2016-12-06 10:02...|/codemove/79SC2H8...|[106.37.189.69, 2...|   3|\n",
            "|{2016-12-06 10:02...|/codemove/LHWWNO3...|[106.37.189.69, 2...|   3|\n",
            "|{2016-12-06 10:01...|/codemove/2J6KRPS...|[106.37.189.69, 2...|   3|\n",
            "+--------------------+--------------------+--------------------+----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}