{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-vinha/SPBD/blob/main/lab2/SPBD_Labs_mapreduce2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKGXsDUIDxH6"
      },
      "source": [
        "# MrJob MapReduce Python Example\n",
        "\n",
        "Word count implemented in pure Python, using the library MrJob.\n",
        "\n",
        "[MrJob](https://mrjob.readthedocs.io/en/latest/) can be used to write MapReduce jobs and run them on several platforms.\n",
        "\n",
        "Some key advantages:\n",
        "+ Write **multi-step** MapReduce jobs in pure Python;\n",
        "+ Test on your **local machine**;\n",
        "+ Deploy jobs in several cloud plataforms of several vendors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "B68ZyGdPDxH9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62b8ebb1-8a43-4d40-c6bd-b64b2a7d484f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/439.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/439.6 kB\u001b[0m \u001b[31m914.0 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m307.2/439.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.6/439.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#@title Download the dataset and install MrJob\n",
        "!wget -q -O os_maias.txt https://www.dropbox.com/s/n24v0z7y79np319/os_maias.txt?dl=0\n",
        "!pip install mrjob --quiet\n",
        "!wget -q -O /etc/mrjob.conf https://raw.githubusercontent.com/smduarte/spbd-2324/main/lab2/mrjob.conf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8sllMoIDxH_"
      },
      "source": [
        "### MrJob WordCount Example\n",
        "Read the words from input and count them.\n",
        "\n",
        "The processing is split into two main phases:\n",
        "\n",
        "+ The mapper emits for each line the number of words\n",
        "+ The reduces sums all the tuples produced by the mapper stage...\n",
        "\n",
        "Using MrJob, a MapReduce job can be expressed in a single Python class,\n",
        "with methods for each of the phases. The reducer phase is called separately for each key, with the collection of values to be reduced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbItx5zwDxIA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a073606f-c90f-4266-b9ef-27a29f92e815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing wordcount.py\n"
          ]
        }
      ],
      "source": [
        "%%file wordcount.py\n",
        "\n",
        "import string\n",
        "from mrjob.job import MRJob\n",
        "\n",
        "class MRWordCount(MRJob):\n",
        "\n",
        "    def mapper(self, _, line):\n",
        "      # remove leading and trailing whitespace\n",
        "      line = line.strip()\n",
        "      # remove punctuation characters\n",
        "      line = line.translate(str.maketrans('', '', string.punctuation+'«»'))\n",
        "      # split the line into words\n",
        "      yield \"words\", len(line.split())\n",
        "\n",
        "    def reducer(self, key, values):\n",
        "        yield key, sum(values)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    MRWordCount.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stOA47uRDxIC"
      },
      "source": [
        "## Execution of MrJob programs\n",
        "\n",
        "### Local Execution"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wordcount\n",
        "\n",
        "# prepare the mapreduce job for local execution\n",
        "mr_job = wordcount.MRWordCount(args=['-r', 'local','os_maias.txt'])\n",
        "\n",
        "# execute the job and print the output results\n",
        "with mr_job.make_runner() as runner:\n",
        "    runner.run()\n",
        "    for key, value in mr_job.parse_output(runner.cat_output()):\n",
        "        print( key, value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6EFN43316on",
        "outputId": "35a7893b-3cc3-4676-ceca-606f982ff9ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "words 213359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supplying a combiner...\n"
      ],
      "metadata": {
        "id": "6IpXfJ-iE-1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file wordcount2.py\n",
        "\n",
        "import string\n",
        "from mrjob.job import MRJob\n",
        "\n",
        "class MRWordCount(MRJob):\n",
        "\n",
        "    def mapper(self, _, line):\n",
        "      # remove leading and trailing whitespace\n",
        "      line = line.strip()\n",
        "      # remove punctuation characters\n",
        "      line = line.translate(str.maketrans('', '', string.punctuation+'«»'))\n",
        "      # split the line into words\n",
        "      yield \"words\", len(line.split())\n",
        "\n",
        "    def combiner(self, key, values):\n",
        "        yield key, sum(values)\n",
        "\n",
        "    def reducer(self, key, values):\n",
        "        yield key, sum(values)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    MRWordCount.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjYU2QVuFLlJ",
        "outputId": "f38b9110-08ce-4955-eb80-f3b0d27f39a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing wordcount2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wordcount2\n",
        "\n",
        "# prepare the mapreduce job for local execution\n",
        "mr_job = wordcount2.MRWordCount(args=['-r', 'local','os_maias.txt'])\n",
        "\n",
        "# execute the job and print the output results\n",
        "with mr_job.make_runner() as runner:\n",
        "    runner.run()\n",
        "    for key, value in mr_job.parse_output(runner.cat_output()):\n",
        "        print( key, value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaVofcsCFSx0",
        "outputId": "c89c97b3-03b3-464f-b951-60c2d0e531e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "words 213359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MrJob Hadoop Execution\n",
        "\n",
        "MrJob can also deploy jobs to run in an Hadoop cluster. The main difference is that the input files and the output results would need to be stored in HDFS (Hadoop distributed filesystem).\n",
        "\n",
        "For now, we will use MrJob in local mode, running in a single machine and its local filesystem."
      ],
      "metadata": {
        "id": "MMu6OTgpKFLS"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}