{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-vinha/SPBD/blob/main/Projeto_SPBD_finalfinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SPBD-23/24 Project Assignment\n",
        "#### version 1.0 - 15/11 (Final)\n",
        "\n",
        "\n",
        "## GROUP - Ana Carolina Condez (67145); Duarte Vinha (67175); Katarina Dyreby (67156)\n",
        "\n",
        "\n",
        "The project scenario involves a dataset of taxi rides, collected December 2022, in the New York city area.\n",
        "\n",
        "Each completed taxi ride corresponds to an event in the dataset. A ride comprises several items of information, including the pick-up and drop-off zones/regions within NY City, their respective timestamps, as well as information related to the payment and number of passengers reported by the driver. The full explanation of the available data is provided [here](https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf).\n",
        "\n",
        "A table to convert zone identifiers into proper names is found [here](https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv).\n",
        "\n",
        "The project assignment will comprise a set of queries. All must be solved using Spark SQL Dataframes API. One query **of your choice** needs to be solved twice more, using Spark Core (mandatory) and, either using the SQL flavor of SparkSQL or using MrJOB.\n",
        "\n",
        "\n",
        "# Deadline\n",
        " + 8th December - 23h59\n",
        " + For each day late, a penalty of 0.5/20 grade points applies."
      ],
      "metadata": {
        "id": "IRibc6b3mULe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download Dataset and Taxi Zone Information\n",
        "!wget -q -O taxirides.csv.gz https://shorturl.at/mzHKY\n",
        "!wget -q -O taxi+_zone_lookup.csv https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv"
      ],
      "metadata": {
        "id": "b7vlO1ERgAkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install PySpark\n",
        "!pip install --quiet pyspark"
      ],
      "metadata": {
        "id": "Wm12pEqlZc9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Queries"
      ],
      "metadata": {
        "id": "BbUk8FJ4eF0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1 - Basic Statistics\n",
        "\n",
        "Compute for each day of the week, the total number of rides, the average ride duration, cost and distance travelled.\n",
        "\n",
        "+ We chose to resolve this question thrice, as requested, using Spark SQL Dataframes API, Spark SQL, and Spark Core."
      ],
      "metadata": {
        "id": "GbRJZX4UeIbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The computation for each day of the week involves an initial step of extracting the weekday from the date and calculating the trip durations in seconds. Following this, the ride data is aggregated based on the day of the week, allowing for the calculation of average ride duration, cost, and distance traveled.\n",
        "\n",
        "After aggregating the ride data according to the day of the week, the days are mapped, enabling a structured presentation of the data ordered by the day of the week."
      ],
      "metadata": {
        "id": "7xNytyKZej0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Basic Statistics using SparkSQL Dataframes API\n",
        "\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]').appName('NYCtaxis').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try :\n",
        "\n",
        "    #Read Data\n",
        "    rides = spark.read.csv('taxirides.csv.gz', header=True, inferSchema=True)\n",
        "\n",
        "    #Extracting the day of the week from pickup datetime\n",
        "    rides = rides.withColumn('pickup_day_of_week', date_format('tpep_pickup_datetime', 'EEEE'))\n",
        "\n",
        "    #Converting pickup and dropoff datetime to UNIX timestamp\n",
        "    rides = rides.withColumn('tpep_pickup_datetime_unix', unix_timestamp('tpep_pickup_datetime'))\n",
        "    rides = rides.withColumn('tpep_dropoff_datetime_unix', unix_timestamp('tpep_dropoff_datetime'))\n",
        "\n",
        "    #Calculating ride duration in seconds\n",
        "    rides = rides.withColumn('ride_duration', abs(col('tpep_dropoff_datetime_unix') - col('tpep_pickup_datetime_unix')))\n",
        "    rides = rides.drop('tpep_pickup_datetime_unix','tpep_dropoff_datetime_unix')\n",
        "\n",
        "    #Aggregating ride data based on day of the week\n",
        "    solution_not_ord = rides.groupBy('pickup_day_of_week') \\\n",
        "        .agg(count('*').alias('total_rides'),\n",
        "        (round(avg('ride_duration'), 4)).alias('avg_ride_duration'),\n",
        "        (round(avg('total_amount'),4)).alias('avg_ride_cost'),\n",
        "        (round(avg('trip_distance'),4)).alias('avg_ride_distance'))\n",
        "\n",
        "    # Mapping weekdays to numerical values\n",
        "    day_map = {'Sunday': 0, 'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5, 'Saturday': 6}\n",
        "    map_weekday = udf(lambda x: day_map[x], IntegerType())\n",
        "    solution_not_ord = solution_not_ord.withColumn('weekday_order', map_weekday(col('pickup_day_of_week')))\n",
        "\n",
        "    #Ordering the resulting DataFrame by weekday in ascending order\n",
        "    solutionq1 = solution_not_ord.orderBy('weekday_order', ascending=True)\n",
        "    solutionq1 = solutionq1.drop('weekday_order')\n",
        "\n",
        "\n",
        "    #Displaying the schema and contents of the resulting DataFrame\n",
        "    solutionq1.printSchema()\n",
        "    solutionq1.show()\n",
        "\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "finally:\n",
        "    sc.stop()\n"
      ],
      "metadata": {
        "id": "vif4LjVx3cmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Basic Statistics using SparkSQL\n",
        "# Compute for each day of the week, the total number of rides, the average ride\n",
        "# duration, cost and distance travelled.\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import unix_timestamp, date_format\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]').appName('NYCtaxis').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try:\n",
        "    #Read the data\n",
        "    rides = spark.read.csv(path=\"taxirides.csv.gz\", header=True, inferSchema=True)\n",
        "\n",
        "    #Create a temporary view\n",
        "    rides.createOrReplaceTempView(\"RIDES\")\n",
        "\n",
        "    #SQL query to compute statistics for each day of the week\n",
        "    result = spark.sql(\"\"\"\n",
        "        SELECT\n",
        "            count(*) as total_rides,\n",
        "            avg(total_amount) as avg_ride_cost,\n",
        "            avg(trip_distance) as avg_ride_distance,\n",
        "            avg((unix_timestamp(tpep_dropoff_datetime) - unix_timestamp(tpep_pickup_datetime)))  as avg_ride_duration,\n",
        "            date_format(tpep_pickup_datetime, 'EEEE') as pickup_day_of_week,\n",
        "            dayofweek(tpep_pickup_datetime) as pickup_day_num\n",
        "        FROM rides\n",
        "        GROUP BY pickup_day_of_week, pickup_day_num\n",
        "        ORDER BY pickup_day_num\n",
        "    \"\"\")\n",
        "\n",
        "    #Create a temporary view to select collumns from\n",
        "    result.createOrReplaceTempView(\"solutionq1\")\n",
        "\n",
        "    #Select only the collumns we want\n",
        "    solutionq1 = spark.sql(\"\"\"\n",
        "        SELECT\n",
        "           pickup_day_of_week, total_rides, avg_ride_duration, avg_ride_cost, avg_ride_distance\n",
        "        FROM solutionq1\n",
        "    \"\"\")\n",
        "\n",
        "\n",
        "    #Displaying the result\n",
        "    solutionq1.show()\n",
        "\n",
        "except Exception as err:\n",
        "    print(err)\n",
        "\n",
        "finally:\n",
        "    sc.stop()\n"
      ],
      "metadata": {
        "id": "iO8ZjvWvR-JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Basic Statistics using Spark Core\n",
        "# Compute for each day of the week, the total number of rides, the average ride\n",
        "# duration, cost and distance travelled.\n",
        "\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from datetime import datetime\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]').appName('NYCtaxis').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try :\n",
        "\n",
        "  #Load the Data\n",
        "  rides = sc.textFile('taxirides.csv.gz')\n",
        "\n",
        "  #Select the first line\n",
        "  header = rides.first()\n",
        "\n",
        "  #Remove the header from the RDD\n",
        "  rides = rides.filter( lambda line: line != header)\n",
        "\n",
        "\n",
        "  solutionq1 = rides.map( lambda line: line.split(','))\\\n",
        "                     .map( lambda a : ( a[1], a[2], a[4], a[16] ))\\\n",
        "                     .map( lambda kv: (datetime.strptime(kv[0], '%Y-%m-%d %H:%M:%S').strftime('%A'), ( 1 , datetime.strptime( kv[1], '%Y-%m-%d %H:%M:%S').timestamp() - datetime.strptime( kv[0], '%Y-%m-%d %H:%M:%S').timestamp() , float(kv[2]), float(kv[3]))))\\\n",
        "                     .reduceByKey( lambda a,b : ( a[0] + b[0], a[1] + b[1], a[2] + b[2], a[3] + b[3] ) )\\\n",
        "                     .map( lambda kv : ( kv[0], ( kv[1][0] , kv[1][1] / kv[1][0] , kv[1][3] / kv[1][0], kv[1][2] / kv[1][0] )))\n",
        "\n",
        "  #Function to convert day of the week to a number for sorting later\n",
        "  def day_of_week_to_number(day):\n",
        "      days = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
        "      return days.index(day)\n",
        "\n",
        "\n",
        "  #Sort the rides by day of the week and collect the results\n",
        "  sorted_rides = solutionq1.sortBy(lambda x: day_of_week_to_number(x[0])).collect()\n",
        "\n",
        "\n",
        "  #Print the sorted rides\n",
        "  # for ride in sorted_rides:\n",
        "  #     print(ride)\n",
        "  for ride in sorted_rides:\n",
        "    day_of_week, stats = ride\n",
        "    print(f\"Day of the week: {day_of_week}\")\n",
        "    print(f\"Total Rides: {stats[0]}\")\n",
        "    print(f\"Average Duration: {stats[1]:.2f} seconds\")\n",
        "    print(f\"Average Fare Amount: ${stats[2]:.2f}\")\n",
        "    print(f\"Average Ride Distance: {stats[3]:.2f} miles\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "\n",
        "finally:\n",
        "  sc.stop()"
      ],
      "metadata": {
        "id": "N1zZWHyMyE7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Q2 Top-5 New York **boroughs**\n",
        "\n",
        "Compute the top-5 New York **boroughs** most popular zones for pick-ups and dropoffs, for the whole month and for each day of the week, separately.\n",
        "\n",
        "This question asks for the top 5 boroughs and not locations.\n",
        "\n",
        "As such, we started by dropping the unnecessary columns from our original dataframe ('rides') and creating 2 new columns - 'PUBorough' and 'DOBorough'. We did this by performing a left outer join with 'location_info' based on the condition that 'PULocationID' or 'DOLocationID' (respectively) match the 'LocationID' in that dataframe, including the column 'Borough' from the 'location_info' dataframe in a new resulting dataframe with the name 'PUBorough' or 'DOBorough', respectively.\n",
        "\n",
        "After that, we needed to compute the day of the week for pick-up and drop-off for each ride (which might not always be the same - a small fraction of rides start in one day but end in another). For that, we used the 'dayofweek' SQL function and created 2 new column - 'DoW_PU' and 'DoW_DO', which contain an integer value representing the day of the week, with 1=Sunday and 7=Saturday.\n",
        "\n",
        "With this preprocessed dataframe we were then able to compute and display the top-5 PU and DO boroughs for the whole month.\n",
        "\n",
        "After that, we also calculated the top-5 PU and DO boroughs for each individual day of the week. To achieve this we first defined a mapping from day numbers to day names. After that, we aggregated and pivoted the data to find the top 5 pick-up and drop-off boroughs for each day of the week, and displayed the results in separate tables."
      ],
      "metadata": {
        "id": "ECc0_6d2ailV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import DataFrame\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.master('local[*]').appName('taxis_q2').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try:\n",
        "    # Load the Data\n",
        "    rides = spark.read.csv('taxirides.csv.gz', header=True, inferSchema=True)\n",
        "    location_info = spark.read.csv('taxi+_zone_lookup.csv', header=True, inferSchema=True)\n",
        "\n",
        "     # Step 1 - Select only the relevant columns from 'rides'\n",
        "    selected_columns = [\n",
        "      \"tpep_pickup_datetime\",\n",
        "      \"tpep_dropoff_datetime\",\n",
        "      \"PULocationID\",\n",
        "      \"DOLocationID\"\n",
        "    ]\n",
        "\n",
        "    rides_selected = rides.select(selected_columns)\n",
        "\n",
        "    # Step 2 - Join the pick-up boroughs\n",
        "    rides_with_pickup_borough = rides_selected.join(\n",
        "      location_info.alias(\"pickup_location\"),\n",
        "      rides_selected.PULocationID == F.col(\"pickup_location.LocationID\"),\n",
        "      \"left_outer\"\n",
        "    ).select(\n",
        "      rides_selected[\"*\"],\n",
        "      F.col(\"pickup_location.Borough\").alias(\"PUBorough\")\n",
        "    )\n",
        "\n",
        "    # Step 3 - Join for drop-off boroughs\n",
        "    rides_with_boroughs = rides_with_pickup_borough.join(\n",
        "      location_info.alias(\"dropoff_location\"),\n",
        "      rides_with_pickup_borough.DOLocationID == F.col(\"dropoff_location.LocationID\"),\n",
        "      \"left_outer\"\n",
        "    ).select(\n",
        "      rides_with_pickup_borough[\"*\"],\n",
        "      F.col(\"dropoff_location.Borough\").alias(\"DOBorough\")\n",
        "    )\n",
        "\n",
        "    # Step 4 - Add the days of the week for pick-up and drop-off for each ride\n",
        "    # The dayofweek function returns an integer corresponding to the day of the\n",
        "    # week where Sunday = 1 and Saturday = 7\n",
        "    rides_with_days = rides_with_boroughs.withColumn(\"DoW_PU\",\n",
        "                                                    F.dayofweek(\"tpep_pickup_datetime\"))\n",
        "    rides_with_days = rides_with_days.withColumn(\"DoW_DO\",\n",
        "                                                F.dayofweek(\"tpep_dropoff_datetime\"))\n",
        "\n",
        "    # Step 5 - Drop the original timestamp columns which are no longer needed\n",
        "    rides_dow_boroughs = rides_with_days.drop(\n",
        "        \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\")\n",
        "\n",
        "    # Step 6 - Compute the top 5 pick-up and drop-off boroughs for the whole month\n",
        "    top5_PU_boroughs_month = rides_dow_boroughs.groupBy(\n",
        "        \"PUBorough\").count().orderBy(F.desc(\"count\")).limit(5)\n",
        "    top5_DO_boroughs_month = rides_dow_boroughs.groupBy(\n",
        "        \"DOBorough\").count().orderBy(F.desc(\"count\")).limit(5)\n",
        "\n",
        "    # Step 7 - Show these results for the whole month\n",
        "    print('Top 5 pick-up boroughs and respective count for the month of December 2022:')\n",
        "    top5_PU_boroughs_month.show()\n",
        "\n",
        "    print('Top 5 drop-off boroughs and respective count for the month of December 2022:')\n",
        "    top5_DO_boroughs_month.show()\n",
        "\n",
        "    # Step 8 - Define a mapping from day numbers to names\n",
        "    day_mapping = {1: \"Sunday\", 2: \"Monday\", 3: \"Tuesday\", 4: \"Wednesday\",\n",
        "                  5: \"Thursday\", 6: \"Friday\", 7: \"Saturday\"}\n",
        "\n",
        "    # Step 9 - Aggregate and pivot for pick-ups and use select to rename the\n",
        "    # columns (so that we get the names of the days in the final result and not\n",
        "    # a number)\n",
        "    top_PU_boroughs = (\n",
        "        rides_dow_boroughs\n",
        "        .groupBy(\"PUBorough\")\n",
        "        .pivot(\"DoW_PU\", range(1, 8))\n",
        "        .agg(F.count(\"PULocationID\"))\n",
        "    )\n",
        "\n",
        "    top_PU_boroughs = top_PU_boroughs.select(\n",
        "      F.col(\"PUBorough\"),\n",
        "      *[F.col(str(day_number)).alias(\n",
        "          day_name) for day_number, day_name in day_mapping.items()]\n",
        "    ).orderBy(*[F.col(\n",
        "        day_name).desc() for day_name in day_mapping.values()]).limit(5)\n",
        "\n",
        "    # Step 10 - A repeat of step 9 but for drop-offs instead\n",
        "    top_DO_boroughs = (\n",
        "        rides_dow_boroughs\n",
        "        .groupBy(\"DOBorough\")\n",
        "        .pivot(\"DoW_DO\", range(1, 8))\n",
        "        .agg(F.count(\"DOLocationID\"))\n",
        "    )\n",
        "\n",
        "    top_DO_boroughs = top_DO_boroughs.select(\n",
        "      F.col(\"DOBorough\"),\n",
        "      *[F.col(str(day_number)).alias(\n",
        "          day_name) for day_number, day_name in day_mapping.items()]\n",
        "    ).orderBy(*[F.col(\n",
        "        day_name).desc() for day_name in day_mapping.values()]).limit(5)\n",
        "\n",
        "    # Step 11 - Show the results for pick-ups and drop-offs for each day of the\n",
        "    # week (in a single table for each case)\n",
        "    print('Top 5 pick-up boroughs for each day of the week and respective count:')\n",
        "    top_PU_boroughs.show()\n",
        "    print('Top 5 drop-off boroughs for each day of the week and respective count:')\n",
        "    top_DO_boroughs.show()\n",
        "\n",
        "except Exception as err:\n",
        "    print(err)\n",
        "\n",
        "finally:\n",
        "    sc.stop()"
      ],
      "metadata": {
        "id": "zOQvbHE4AzcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3 - Compute a list of anomalous rides.\n",
        "\n",
        "Anomalous rides are those that deviate, significantly, either in terms of cost or distance travelled, from rides that started and ended in the same zone."
      ],
      "metadata": {
        "id": "RZi8z1gVdSGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To identify anomalous rides, we begin by initializing an empty list that will hold the resulting anomalous rides. Each ride is uniquely identified by assigning a unique identifier to all rows in the dataset.\n",
        "\n",
        "A new dataframe is then created, computing the average and standard deviation of the trip distance and total amount for rides between specific pickup and drop-off locations. This information is merged with the initial dataframe, ensuring that each row now contains the average and standard deviation for trip distance and total amount concerning rides with the same pickup and drop-off locations.\n",
        "\n",
        "Two z-scores are calculatedâ€”one for trip distance and another for the total amount. A threshold of 3 is chosen; any z-score above or below the absolute value of 3 is considered an outlier. Rides exhibiting z-scores beyond this threshold are marked as anomalous.\n",
        "\n",
        "Finally, the identified anomalous rides are stored in the list created at the beginning."
      ],
      "metadata": {
        "id": "qFx18RfGf9nC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]').appName('NYCtaxis').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try :\n",
        "    # Make an empty list to later store anomalous rides\n",
        "    anomalous_rides_id_list = []\n",
        "\n",
        "    # Read the data\n",
        "    rides = spark.read.csv('taxirides.csv.gz', header=True, inferSchema=True)\n",
        "\n",
        "    # Assign unique IDs to each ride\n",
        "    rides = rides.withColumn('ride_id', monotonically_increasing_id())\n",
        "\n",
        "    # Calculate average and standard deviation of trip distance and total amount for rides\n",
        "    rides_same_location = rides.select('total_amount','Trip_distance', 'PULocationID','DOLocationID')\\\n",
        "             .groupBy('PULocationID','DOLocationID')\\\n",
        "             .agg(avg('Trip_distance').alias('avg_Trip_distance'), avg('total_amount').alias('avg_total_amount'),stddev('Trip_distance').alias('stddev_Trip_distance'), stddev('total_amount').alias('stddev_total_amount'))\n",
        "\n",
        "    # Join the ride data with average and standard deviation data\n",
        "    rides_with_avg = rides.join(rides_same_location,['PULocationID', 'DOLocationID'],'left')\n",
        "\n",
        "\n",
        "    # Calculate z-scores for trip distance and total amount\n",
        "    rides_with_zscores = rides_with_avg.withColumn('Trip_distance_zscore',(col('Trip_distance') - col('avg_Trip_distance')) / col('stddev_Trip_distance')).withColumn('total_amount_zscore',(col('total_amount') - col('avg_total_amount')) / col('stddev_total_amount'))\n",
        "\n",
        "    # Define threshold for z-score outliers\n",
        "    threshold = 3\n",
        "\n",
        "    # Select ride IDs of anomalous rides based on the threshold\n",
        "    anomalous_rides = rides_with_zscores.filter(((col('total_amount_zscore') > threshold) & (col('trip_distance_zscore') > threshold))|((col('total_amount_zscore') < -threshold) & (col('trip_distance_zscore') < -threshold))|((col('total_amount_zscore') > threshold) & (col('trip_distance_zscore') < -threshold))|((col('total_amount_zscore') < -threshold) & (col('trip_distance_zscore') > threshold)))\n",
        "\n",
        "    # Select only the anomalous ride_id collumn\n",
        "    anomalous_rides_id = anomalous_rides.select('ride_id')\n",
        "\n",
        "    # Append all anomalous ride ids to the original list\n",
        "    for row in anomalous_rides_id.collect():\n",
        "        anomalous_rides_id_list.append(row[0])\n",
        "\n",
        "    # Print the list of anomalous ride IDs\n",
        "    print(anomalous_rides_id_list)\n",
        "\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "  sc.stop()"
      ],
      "metadata": {
        "id": "1AzxfN6TLDL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4 - Find out which zones tend to generate shorter rides and which generate longer rides.\n",
        "\n",
        " Consider a ride short or long, respectively, if it less or more than 30% than the average distance for rides that originate in that zone."
      ],
      "metadata": {
        "id": "kZTbIQhndWha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to determine which zones tend to generate shorter or longer rides, the process begins by computing the average distance for each 'PULocationID'. Two empty lists are initialized to hold the solutions for shorter and longer ride-generating zones.\n",
        "\n",
        "Next, the newly calculated average distances are merged with the original ride dataframe, ensuring each row now includes the average distance specific to its 'PULocationID'. Then, a new dataframe is created to categorize the rides into three sections: Short, Medium, or Long. This classification is based on whether the ride distance is less or more than 30% of the average distance for rides originating in that zone.\n",
        "\n",
        "Following this, the counts of short and long rides originating from each 'PULocationID' are computed. If a 'PULocationID' has more short rides than long ones, it tends to generate shorter rides; conversely, if it has more long rides, it tends to generate longer rides.\n",
        "\n",
        "Finally, the identified 'PULocationID's that tend to generate short or long rides are stored in two separate lists."
      ],
      "metadata": {
        "id": "bgKTkrktgJz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Consider a ride short or long, respectively, if it less or more than 30% than\n",
        "# the average distance for rides that originate in that zone.\n",
        "\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]').appName('NYCtaxis').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try :\n",
        "    #Make two empty lists to later store the results\n",
        "    long_rides = []\n",
        "    short_rides = []\n",
        "\n",
        "    #Read the data\n",
        "    rides = spark.read.csv('taxirides.csv.gz', header=True, inferSchema=True)\n",
        "\n",
        "    #Calculate average trip distance per PULocationID\n",
        "    rides_pu_trip_distance = rides.select('PULocationID','trip_distance').groupBy('PULocationID').agg(avg('trip_distance').alias('avg_trip_distance'))\n",
        "\n",
        "    #Join the average trip distance with the original rides dataframe\n",
        "    rides_with_avg = rides.join(rides_pu_trip_distance,['PULocationID'],'left')\n",
        "\n",
        "    rides_with_long_short = rides_with_avg.withColumn('ride_length',\n",
        "                                                      when(col('trip_distance') < col('avg_trip_distance') * 0.7, 'Short')\n",
        "                                                      .when(col('trip_distance') > col('avg_trip_distance') * 1.3, 'Long')\n",
        "                                                      .otherwise('Medium'))\n",
        "\n",
        "    #Determine if a ride is short, long, or medium based on average trip distance\n",
        "    rides_with_long_short = rides_with_long_short.select('PULocationID', 'ride_length') \\\n",
        "                        .groupBy('PULocationID') \\\n",
        "                        .agg(sum(when(col('ride_length') == 'Short', 1).otherwise(0)).alias('count_short'),\n",
        "                         sum(when(col('ride_length') == 'Long', 1).otherwise(0)).alias('count_long'))\n",
        "\n",
        "    #Group by PULocationID to count short and long rides\n",
        "    rides_with_long_short = rides_with_long_short.withColumn('tends_to_generate', when(col('count_short') > col('count_long'), 'Short').otherwise('Long'))\n",
        "    rides_with_long_short = rides_with_long_short.select('PULocationID','tends_to_generate')\n",
        "\n",
        "    #Collect results and appened them to the short_rides and long_rides lists\n",
        "    for row in rides_with_long_short.collect():\n",
        "        if row['tends_to_generate'] == 'Short':\n",
        "            short_rides.append(row['PULocationID'])\n",
        "        else:\n",
        "            long_rides.append(row['PULocationID'])\n",
        "\n",
        "\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "  sc.stop()\n",
        "\n"
      ],
      "metadata": {
        "id": "UXcGQj5xK1ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Q4 printing the results with zone name\n",
        "\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]').appName('NYCtaxis_Q4').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try :\n",
        "    # Make two empty lists to later store the results\n",
        "    long_rides = []\n",
        "    short_rides = []\n",
        "\n",
        "    # Read the data\n",
        "    rides = spark.read.csv('taxirides.csv.gz', header=True, inferSchema=True)\n",
        "    location_info = spark.read.csv('taxi+_zone_lookup.csv', header=True,\n",
        "                                   inferSchema=True)\n",
        "\n",
        "\n",
        "    # Calculate average trip distance per PULocationID\n",
        "    rides_pu_trip_distance = rides.select('PULocationID','trip_distance').groupBy('PULocationID').agg(avg('trip_distance').alias('avg_trip_distance'))\n",
        "\n",
        "    # Join the average trip distance with the original rides dataframe\n",
        "    rides_with_avg = rides.join(rides_pu_trip_distance,['PULocationID'],'left')\n",
        "\n",
        "    rides_with_long_short = rides_with_avg.withColumn('ride_length',\n",
        "                                                      when(col('trip_distance') < col('avg_trip_distance') * 0.7, 'Short')\n",
        "                                                      .when(col('trip_distance') > col('avg_trip_distance') * 1.3, 'Long')\n",
        "                                                      .otherwise('Medium'))\n",
        "\n",
        "    # Determine if a ride is short, long, or medium based on average trip distance\n",
        "    rides_with_long_short = rides_with_long_short.select('PULocationID', 'ride_length') \\\n",
        "                        .groupBy('PULocationID') \\\n",
        "                        .agg(sum(when(col('ride_length') == 'Short', 1).otherwise(0)).alias('count_short'),\n",
        "                         sum(when(col('ride_length') == 'Long', 1).otherwise(0)).alias('count_long'))\n",
        "\n",
        "    # Group by PULocationID to count short and long rides\n",
        "    rides_with_long_short = rides_with_long_short.withColumn('tends_to_generate', when(col('count_short') > col('count_long'), 'Short').otherwise('Long'))\n",
        "    rides_with_long_short = rides_with_long_short.select('PULocationID','tends_to_generate')\n",
        "\n",
        "    # Join with location_info to get the Zone Name\n",
        "    rides_with_zone = rides_with_long_short.join(location_info, rides_with_long_short['PULocationID'] == location_info['LocationID'])\n",
        "\n",
        "    # Collect results and append them to the short_rides and long_rides lists\n",
        "    sorted_results = rides_with_zone.sort('PULocationID').collect()\n",
        "\n",
        "    for row in sorted_results:\n",
        "        zone_name = row['Zone']\n",
        "        if row['tends_to_generate'] == 'Short':\n",
        "            short_rides.append(row['PULocationID'])\n",
        "        else:\n",
        "            long_rides.append(row['PULocationID'])\n",
        "        print(f\"Pick-up location with ID: {row['PULocationID']} ({zone_name}) tends to generate: {row['tends_to_generate']} rides\")\n",
        "\n",
        "except Exception as err:\n",
        "    print(err)\n",
        "\n",
        "finally:\n",
        "    sc.stop()"
      ],
      "metadata": {
        "id": "mv_8Wvw8l7YE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5 - Find most important city zones using the Pagerank metric\n",
        "Consider the graph where locations/zones (vertices) are connected by the taxi rides (edges). Locations that have many incoming rides, ie., those that are the dropoff location for many rides, will tend to be important hubs (centers of activity) in the city. Use Pagerank to find these hubs.\n",
        "\n",
        "To that end, to simplify the graph, do not consider rides that involve \"Unknown\" zones. Additionally, for each zone, only consider the rides that start in that zone and end in the top-5 destinations for that zone (This will remove the edges corresponding to (src-dst) zone pairs that are not very popular).\n",
        "\n",
        "Use the [GraphFrames API](https://graphframes.github.io/graphframes/docs/_site/index.html) and check the example below for a simple PageRank computation."
      ],
      "metadata": {
        "id": "QwDKjqGqdamO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install GraphFrames\n",
        "!pip install --quiet graphframes"
      ],
      "metadata": {
        "id": "VEcyJvs4dcub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to find the most important zones of the city, according to the defined criteria, we start by filtering our DataFrame, eliminating the rows where either \"PULocationID\", \"DOLocationID\" or both have the IDs 264 or 265, as those are the codes corresponding to \"Unknown\" zones.\n",
        "\n",
        "Next, we have to define our vertices. This involves selecting the relevant columns - the pick-up and drop-off location IDs (\"PULocationID\", \"DOLocationID\") from the filtered DataFrame.\n",
        "\n",
        "After this, we need to obtain the top-5 destinations (\"DOLocationID\") for each pick-up zone (\"PULocationID\"). To do this, we defined a window specification to rank the drop-off locations based on the number of rides originating from each pick-up zone. Using this window specification, we then computed the top-5 destinations for each zone and created a DataFrame containing these high-frequency destinations.\n",
        "\n",
        "Subsequently, we created our edges. To that aim, we filtered them using the top-destinations DataFrame, ensuring that only edges connecting zones with high-frequency destinations are retained. We also renamed the columns \"PULocationID\" and \"DOLocationID\" to \"src\" and \"dst,\" respectively, conforming to the GraphFrame API convention.\n",
        "\n",
        "Following this, we created a GraphFrame object using the filtered vertices and edges DataFrames.\n",
        "\n",
        "Afterwards, we applied the PageRank algorithm to the GraphFrame, setting the reset probability to 0.15 (as is used in the original PageRank algorithm) and the maximum number of iterations to 50 (to set a cap if no convergence has been reach by that iteration).\n",
        "\n",
        "Then, we retrieved our results, sorting them in descending order according to the PageRank score of each location. Seen as the locations in our original data (rides) are only represented by their respective \"LocationID\", and that data for correspondence to actual city boroughs and zones was actually available in a separate .csv file, we aggregated this information to our results, to make them more interpretable.\n",
        "\n",
        "Finally, we also ranked the NYC Boroughs according to the computed mean PageRank of each borough (taking into account the number of LocationIDs that each borough encompasses) to also understand which boroughs are more important."
      ],
      "metadata": {
        "id": "w1yX3SjbZy2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import functions as F\n",
        "from graphframes import *\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "        .config('spark.jars.packages',\n",
        "                'graphframes:graphframes:0.8.3-spark3.5-s_2.12') \\\n",
        "        .appName('Graphframes NYC Taxi Rides').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try:\n",
        "  # Read the data\n",
        "  rides = spark.read.csv('taxirides.csv.gz', header=True, inferSchema=True)\n",
        "  location_info = spark.read.csv('taxi+_zone_lookup.csv',\n",
        "                                 header=True, inferSchema=True)\n",
        "\n",
        "  # Step 1 - Filter out rides involving \"Unknown\" zones, which have ids 264 or 265\n",
        "  filtered_taxi_df = rides.filter(\n",
        "    (F.col(\"PULocationID\").isin([264, 265]) == False) &\n",
        "    (F.col(\"DOLocationID\").isin([264, 265]) == False)\n",
        ")\n",
        "\n",
        "  # Step 2 - Select relevant columns for vertices\n",
        "  vertices = (\n",
        "    filtered_taxi_df\n",
        "    .selectExpr(\"PULocationID as id\")\n",
        "    .union(filtered_taxi_df.selectExpr(\"DOLocationID as id\"))\n",
        "    .distinct()\n",
        "  )\n",
        "\n",
        "  # Step 3 - Define a window specification for ranking\n",
        "  window_spec = Window.partitionBy(\"PULocationID\").orderBy(F.col(\n",
        "      \"count\").desc())\n",
        "\n",
        "  # Step 4 - Compute the top-5 destinations for each zone\n",
        "  top_destinations = (\n",
        "    filtered_taxi_df\n",
        "    .groupBy(\"PULocationID\", \"DOLocationID\")\n",
        "    .count()\n",
        "    .withColumn(\"rank\", F.row_number().over(window_spec))\n",
        "    .filter(F.col(\"rank\") <= 5)\n",
        "    .drop(\"count\", \"rank\")\n",
        "  )\n",
        "\n",
        "  # Step 5 - Use the top-5 destinations to filter edges and rename columns to\n",
        "  # 'src' and 'dst', to conform to the GraphFrames API convention\n",
        "  edges = (\n",
        "    filtered_taxi_df\n",
        "    .join(top_destinations, [\"PULocationID\", \"DOLocationID\"], \"inner\")\n",
        "    .select(\"PULocationID\", \"DOLocationID\")\n",
        "    .distinct()\n",
        "    .withColumnRenamed(\"PULocationID\", \"src\")\n",
        "    .withColumnRenamed(\"DOLocationID\", \"dst\")\n",
        "  )\n",
        "\n",
        "  # Step 6 - Create a Graph using GraphFrame\n",
        "  graph = GraphFrame(vertices, edges)\n",
        "\n",
        "  # Step 7 - Run the PageRank Algorithm\n",
        "  pagerank_results = graph.pageRank(resetProbability=0.15, maxIter=50)\n",
        "\n",
        "  # Step 8 - Join the PageRank results with the location information to get the\n",
        "  # actual names of the Boroughs and Zones instead of only ids\n",
        "  result_with_location_info = (\n",
        "      pagerank_results.vertices\n",
        "      .join(location_info, pagerank_results.vertices[\"id\"] == location_info[\n",
        "          \"LocationID\"], \"left\")\n",
        "      .select(\n",
        "          pagerank_results.vertices[\"id\"],\n",
        "          location_info[\"Borough\"].alias(\"Borough\"),\n",
        "          location_info[\"Zone\"].alias(\"Zone\"),\n",
        "          pagerank_results.vertices[\"pagerank\"]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  # Step 9 - Get the final PageRank results with Borough and Zone information\n",
        "  print('Most Important Locations of NYC according to PageRank\\\n",
        "  metric with Zone name and Borough:')\n",
        "  result_with_location_info.orderBy(\"pagerank\", ascending=False).show()\n",
        "\n",
        "  # Step 10 - Find out also which are the most relevant Boroughs by mean\n",
        "  # PageRank according to how many LocationIDs each Borough contains\n",
        "  mean_pagerank_per_borough = (\n",
        "      result_with_location_info\n",
        "      .groupBy(\"Borough\")\n",
        "      .agg(\n",
        "          F.sum(\"pagerank\").alias(\"sum_pagerank\"),\n",
        "          F.countDistinct(\"id\").alias(\"num_ids\")\n",
        "      )\n",
        "      .withColumn(\"mean_pagerank\", F.col(\"sum_pagerank\") / F.col(\"num_ids\"))\n",
        "      .orderBy(\"mean_pagerank\", ascending=False)\n",
        "  )\n",
        "\n",
        "  # Step 11 - View the mean PageRank results for boroughs\n",
        "  print('Most Important Boroughs of NYC by mean PageRank metric:')\n",
        "  mean_pagerank_per_borough.show()\n",
        "\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "\n",
        "finally:\n",
        "  sc.stop()"
      ],
      "metadata": {
        "id": "OdT6nvd1Z8EM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}