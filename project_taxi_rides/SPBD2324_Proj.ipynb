{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-vinha/SPBD/blob/main/project_taxi_rides/SPBD2324_Proj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SPBD-2324 Project Assignment\n",
        "\n",
        "#### version 0.1 - 18/10\n",
        "\n",
        "The project scenario involves a dataset of taxi rides, collected December 2022, in the New York city area.\n",
        "\n",
        "Each completed taxi ride corresponds to an event in the dataset. A ride comprises several items of information, including the pick-up and drop-off zones/regions within NY City, their respective timestamps, as well as information related to the payment and number of passengers reported by the driver. The full explanation of the available data is provided [here](https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf).\n",
        "\n",
        "A table to convert zone identifiers into proper names is found [here](https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv).\n",
        "\n",
        "The project assignment will comprise a set of queries. All must be solved using Spark SQL Dataframes API. One query **of your choice** needs to be solved twice more, using Spark Core (mandatory) and, either using the SQL flavor of SparkSQL or using MrJOB.\n",
        "\n",
        "# Queries\n",
        "\n",
        "## Q1 - Basic Statistics\n",
        "\n",
        "Compute for each day of the week, the total number of rides, the average ride duration, cost and distance travelled.\n",
        "\n",
        "## Q2 Top-5 New York **boroughs**\n",
        "\n",
        "Compute the top-5 New York **boroughs** most popular zones for pick-ups and dropoffs, for the whole month and for each day of the week, separately.\n",
        "\n",
        "## Q3 - Compute a list of anomalous rides.\n",
        "\n",
        "Anomalous rides are those that deviate, significantly, either in terms of cost or distance travelled, from rides that started and ended in the same zone.\n",
        "\n",
        "## Q4 - Find the which zones tend to generate shorter rides and which generate longer rides.\n",
        "\n",
        " Consider a ride short or long, respectively, if it less or more than 30% than the average distance for rides that originate in that zone.\n",
        "\n",
        "## Q5 - To be defined...\n",
        "\n",
        "# Deadline\n",
        " + 8th December - 23h59\n",
        " + For each day late, a penalty of 0.5/20 grade points applies."
      ],
      "metadata": {
        "id": "IRibc6b3mULe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Code\n",
        "\n",
        "The cells below show how to download the dataset and\n",
        "start processing it using Spark Core and SparkSQL."
      ],
      "metadata": {
        "id": "75rHfXd64EbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Dataset\n",
        "!wget -q -O taxirides.csv.gz https://shorturl.at/mzHKY"
      ],
      "metadata": {
        "id": "b7vlO1ERgAkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install PySpark\n",
        "!pip install --quiet pyspark"
      ],
      "metadata": {
        "id": "Wm12pEqlZc9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Spark Core Example\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]').appName('NYCtaxis').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try :\n",
        "  rides = sc.textFile('taxirides.csv.gz')\n",
        "\n",
        "  for ride in rides.take(10):\n",
        "    print(ride)\n",
        "\n",
        "  sc.stop()\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  sc.stop()"
      ],
      "metadata": {
        "id": "tJDZB0KxZK9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SparkSQL Example\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]').appName('NYCtaxis').getOrCreate()\n",
        "\n",
        "try :\n",
        "  trips = spark.read.csv(path = \"taxirides.csv.gz\", header= True, inferSchema= True )\n",
        "  trips.printSchema()\n",
        "  trips.show(5)\n",
        "\n",
        "except Exception as err:\n",
        "  print(err)"
      ],
      "metadata": {
        "id": "qz0zswfyZara"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QUESTION 1 - BASIC STATISTICS"
      ],
      "metadata": {
        "id": "5-a6z1M-6sy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]').appName('NYCtaxis').getOrCreate()\n",
        "\n",
        "try :\n",
        "  trips = spark.read.csv(path = \"taxirides.csv.gz\", header= True, inferSchema= True )\n",
        "  trips.printSchema()\n",
        "  trips.createOrReplaceTempView(\"TAXI_TRIPS_NYC\")\n",
        "\n",
        "\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9NqQ3FIg68ST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the DataFrame as a temporary view\n",
        "trips.createOrReplaceTempView(\"TAXI_TRIPS_NYC\")\n",
        "\n",
        "# Execute SQL query to order the rows by passenger_count\n",
        "trips_weekdays = spark.sql(\"SELECT *, DATE_FORMAT(tpep_pickup_datetime, 'EEEE') AS weekday FROM TAXI_TRIPS_NYC\")\n",
        "\n",
        "# Save to use for SQL statements\n",
        "trips_weekdays.createOrReplaceTempView(\"TRIPS_WEEKDAYS\")\n",
        "\n",
        "# SQL query to drop unnecessary columns\n",
        "columns_to_keep = \"tpep_pickup_datetime, tpep_dropoff_datetime, trip_distance, \\\n",
        "                  total_amount, congestion_surcharge, airport_fee, weekday\"\n",
        "query = f\"SELECT {columns_to_keep} FROM TRIPS_WEEKDAYS\"\n",
        "\n",
        "# Create a new DataFrame selecting specific columns\n",
        "trips_weekdays_redux = spark.sql(query)\n",
        "\n",
        "trips_weekdays_redux.createOrReplaceTempView(\"TRIPS_WEEKDAYS_REDUX\")\n",
        "\n",
        "# SQL query to calculate ride_cost and add it as a new column\n",
        "query = \"\"\"\n",
        "    SELECT *,\n",
        "           (total_amount + congestion_surcharge + airport_fee) AS ride_cost\n",
        "    FROM TRIPS_WEEKDAYS_REDUX\n",
        "\"\"\"\n",
        "\n",
        "# Create a new DataFrame with the ride_cost column\n",
        "trips_weekdays_ride_cost = spark.sql(query)\n",
        "\n",
        "# Show the DataFrame with the new ride_cost column\n",
        "trips_weekdays_ride_cost.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "k5VarmDKABgv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}