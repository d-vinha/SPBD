{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-vinha/SPBD/blob/main/lab4/SPBD_Labs_spark1_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtsPM1Z4HH7M"
      },
      "source": [
        "# Python Spark Exercises\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Pyspark\n",
        "!pip install --quiet pyspark"
      ],
      "metadata": {
        "id": "BuFS4wO2B1vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Sorted Word Frequency\n",
        "\n",
        "Create a [Spark](https://spark.apache.org/docs/latest/api/python/) program that counts the number of occurrences of each word in \"Os Maias\" novel, sorting them by frequency (the words with higher occurrence first).\n",
        "\n",
        "Note that the sorting should be performed as a transformation (i.e. it should produce an RDD)..."
      ],
      "metadata": {
        "id": "f7oXYnylGyko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download \"Os Maias\"\n",
        "!wget -q -O os_maias.txt https://www.dropbox.com/s/n24v0z7y79np319/os_maias.txt?dl=0"
      ],
      "metadata": {
        "id": "GEn0_HxQHDlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1) Create a Spark program that counts the number of occurrences of each word in \"Os Maias\" novel, sorting them by frequency (the words with higher occurrence first).\n",
        "\n",
        "Note that the sorting should be performed as a transformation (i.e. it should produce an RDD)..."
      ],
      "metadata": {
        "id": "DOnpjUCPaZ5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "import string\n",
        "\n",
        "sc = pyspark.SparkContext('local[*]')\n",
        "try:\n",
        "  lines = sc.textFile('os_maias.txt') \\\n",
        "      .map( lambda line: line.strip() ) \\\n",
        "      .map( lambda word: word.lower() ) \\\n",
        "      .map( lambda line: line.translate(str.maketrans('', '', string.punctuation+'«»')) ) \\\n",
        "\n",
        "\n",
        "  words = lines.flatMap( lambda line: line.split() ) \\\n",
        "          .map( lambda word: (word, 1)) \\\n",
        "          .reduceByKey( lambda a, b: a+b)\n",
        "\n",
        "\n",
        "  sorted_words = words.sortBy(lambda x: x[1], ascending = False)\n",
        "\n",
        "  for w,f in sorted_words.take(100):\n",
        "      print(\"{}\\t{}\".format(w,f))\n",
        "\n",
        "  sc.stop()\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  sc.stop()"
      ],
      "metadata": {
        "id": "6Q5p4vz8e7Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2) Create a Spark program that the top 10 most used words in \"Os Maias\" novel.\n",
        "\n",
        "You should try to avoid sorting or finding the top-10 as actions. Your top-10 most used words should be a RDD at the end of the computation. Check *zipWithIndex* in [pyspark RDD](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html#pyspark.RDD) documentation."
      ],
      "metadata": {
        "id": "yG6RH3d3ab8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "import string\n",
        "\n",
        "sc = pyspark.SparkContext('local[*]')\n",
        "try:\n",
        "  lines = sc.textFile('os_maias.txt') \\\n",
        "      .map( lambda line: line.strip() ) \\\n",
        "      .map( lambda word: word.lower() ) \\\n",
        "      .map( lambda line: line.translate(str.maketrans('', '', string.punctuation+'«»')) ) \\\n",
        "\n",
        "  top10_words_partitions = lines.flatMap( lambda line: line.split() ) \\\n",
        "          .map( lambda word: (word, 1)) \\\n",
        "          .reduceByKey( lambda a, b: a+b) \\\n",
        "          .mapPartitions( lambda partition : sorted(partition, key=lambda kv: kv[1], reverse=True)[0:10])\n",
        "\n",
        "  top10_words = top10_words_partitions.sortBy(lambda x: x[1], ascending = False) \\\n",
        "                .zipWithIndex() \\\n",
        "                .filter( lambda ranked: ranked[1] < 10) \\\n",
        "                .map( lambda ranked: ranked[0])\n",
        "\n",
        "  print(\"Partitions: Top-10\")\n",
        "  for x in top10_words_partitions.glom().collect():\n",
        "      print(\"{}\".format(x))\n",
        "\n",
        "  print(\"Top-10 Most frequent words:\")\n",
        "  for x in top10_words.collect():\n",
        "      print(\"{}\".format(x))\n",
        "\n",
        "  sc.stop()\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  sc.stop()"
      ],
      "metadata": {
        "id": "yUHQq9P5aktT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Weblog Analysis\n",
        "\n",
        "Consider a set of log files captured during a DDOS (*Distributed Denial of Service*) attack, containing information for the web accesses performed during the attack to the server.\n",
        "\n",
        "The log files contain text lines as shown below, with TAB as the separator:\n",
        "\n",
        "date |IP_source | status_code | operation | URL | execution time |\n",
        "-|-|-|-|-|-\n",
        "timestamp  | string | int | string | string| float |\n",
        "2016-12-06T08:58:35.318+0000|37.139.9.11|404|GET|/codemove/TTCENCUFMH3C|0.026"
      ],
      "metadata": {
        "id": "rsJZWYlHZDJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download the dataset\n",
        "!wget -q -O web.log https://www.dropbox.com/s/0r8902uj9yum7dg/web.log?dl=0\n",
        "!head -1 web.log"
      ],
      "metadata": {
        "id": "WCWKj68qCOdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Count the number of unique IP addresses involved in the DDOS attack. Do not use the ***distinct()*** transformation.\n"
      ],
      "metadata": {
        "id": "N1-ojIAqCftf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "\n",
        "sc = pyspark.SparkContext('local[*]')\n",
        "try:\n",
        "  lines = sc.textFile('web.log') \\\n",
        "          .map( lambda line: line.strip() )\n",
        "\n",
        "  unique_ips = lines.map( lambda line: line.split()) \\\n",
        "          .filter( lambda values: len(values) == 6) \\\n",
        "          .map( lambda values : (values[1], None)) \\\n",
        "          .reduceByKey( lambda a, b : None ) \\\n",
        "          .map( lambda _ : (None, 1)) \\\n",
        "          .reduceByKey( lambda a, b : a+b ) \\\n",
        "\n",
        "\n",
        "  for _,c in unique_ips.collect():\n",
        "      print(\"{}\".format(c))\n",
        "\n",
        "  sc.stop()\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  sc.stop()"
      ],
      "metadata": {
        "id": "Y7XoyNETChb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "\n",
        "sc = pyspark.SparkContext('local[*]')\n",
        "try:\n",
        "  lines = sc.textFile('web.log') \\\n",
        "          .map( lambda line: line.strip() )\n",
        "\n",
        "  unique_ips = lines.map( lambda line: line.split()) \\\n",
        "          .filter( lambda values: len(values) == 6) \\\n",
        "          .map( lambda values : values[1] ) \\\n",
        "          .distinct() \\\n",
        "          .map( lambda _ : (None, 1)) \\\n",
        "          .reduceByKey( lambda a, b : a+b ) \\\n",
        "\n",
        "\n",
        "  for _,c in unique_ips.collect():\n",
        "      print(\"{}\".format(c))\n",
        "\n",
        "  sc.stop()\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  sc.stop()"
      ],
      "metadata": {
        "id": "XUQLSHJh-p3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "\n",
        "sc = pyspark.SparkContext('local[*]')\n",
        "try:\n",
        "  lines = sc.textFile('web.log') \\\n",
        "          .map( lambda line: line.strip() )\n",
        "\n",
        "  unique_ips = lines.map( lambda line: line.split()) \\\n",
        "          .filter( lambda values: len(values) == 6) \\\n",
        "          .map( lambda values : values[1] ) \\\n",
        "          .distinct()\n",
        "\n",
        "\n",
        "  print(unique_ips.count())\n",
        "\n",
        "  sc.stop()\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  sc.stop()"
      ],
      "metadata": {
        "id": "aySsc3dFY1Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) For each interval of 10 seconds, provide the following information: [number of requests, average execution time, maximum time, minimum time]"
      ],
      "metadata": {
        "id": "ZJ5TzPdACgQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from operator import *\n",
        "\n",
        "sc = pyspark.SparkContext('local[*]')\n",
        "try:\n",
        "  lines = sc.textFile('web.log') \\\n",
        "         .map( lambda line: line.strip() )\n",
        "\n",
        "  intervals = lines.map( lambda line: line.split()) \\\n",
        "          .filter( lambda values: len(values) == 6) \\\n",
        "          .map( lambda values: (values[0][0:18], float(values[5]))) \\\n",
        "          .map( lambda kv : (kv[0], (1, kv[1], kv[1], kv[1]))) \\\n",
        "          .reduceByKey( lambda a, b : (a[0] + b[0], a[1] + b[1], max(a[2],b[2]), min(a[3],b[3])) ) \\\n",
        "          .map( lambda kv : (kv[0], (kv[1][0], kv[1][1] / kv[1][0], kv[1][2], kv[1][3]))) \\\n",
        "          .sortByKey()\n",
        "\n",
        "  for interval in intervals.take(100):\n",
        "    print(interval)\n",
        "\n",
        "  sc.stop()\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  sc.stop()"
      ],
      "metadata": {
        "id": "M8UVCwcdCwTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Create an inverted index that, for each interval of 10 seconds, has a list of (unique) IPs executing accesses (to each URL)."
      ],
      "metadata": {
        "id": "jUHmctaICgtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "\n",
        "sc = pyspark.SparkContext('local[*]')\n",
        "try:\n",
        "  lines = sc.textFile('web.log') \\\n",
        "          .map( lambda line: line.strip() )\n",
        "\n",
        "\n",
        "  intervals = lines.map( lambda line: line.split()) \\\n",
        "          .filter( lambda values: len(values) == 6) \\\n",
        "          .map( lambda values: (\"{}-{}\".format(values[0][0:18], values[4]), { values[1] } )) \\\n",
        "          .reduceByKey( lambda a, b : a | b ) \\\n",
        "          .sortByKey()\n",
        "\n",
        "  for v in intervals.collect():\n",
        "    print(v)\n",
        "\n",
        "  sc.stop()\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  sc.stop()"
      ],
      "metadata": {
        "id": "RpXghha0C0jC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}