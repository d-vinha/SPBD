{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-vinha/SPBD/blob/main/lab3/SPBD_Labs_spark1_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtsPM1Z4HH7M"
      },
      "source": [
        "# Python Spark Exercises\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Pyspark\n",
        "!pip install --quiet pyspark"
      ],
      "metadata": {
        "id": "BuFS4wO2B1vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download \"Os Maias\"\n",
        "!wget -q -O os_maias.txt https://www.dropbox.com/s/n24v0z7y79np319/os_maias.txt?dl=0"
      ],
      "metadata": {
        "id": "GEn0_HxQHDlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Sorted Word Frequency\n",
        "\n",
        "1.1) Create a [Spark](https://spark.apache.org/docs/latest/api/python/) program that counts the number of occurrences of each word in \"Os Maias\" novel, sorting them by frequency (the words with higher occurrence first).\n",
        "\n",
        "Note that the sorting should be performed as a transformation (i.e. it should produce an RDD)..."
      ],
      "metadata": {
        "id": "f7oXYnylGyko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "\n",
        "sc = pyspark.SparkContext('local[*]')\n",
        "try:\n",
        "  lines = sc.textFile('os_maias.txt')\n",
        "\n",
        "  for line in lines.take(10):\n",
        "    print(line)\n",
        "\n",
        "  sc.stop()\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  sc.stop()"
      ],
      "metadata": {
        "id": "qMFaHhpWHxkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2) Create a Spark program that computes the top 10 most used words in \"Os Maias\" novel.\n",
        "\n",
        "You should try to avoid sorting or finding the top-10 as actions. Your top-10 most used words should be a RDD at the end of the computation. Check *zipWithIndex* in [pyspark RDD](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html#pyspark.RDD) documentation"
      ],
      "metadata": {
        "id": "UkI4QSo8Ua35"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_MmTctRBVu7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "\n",
        "sc = pyspark.SparkContext('local[*]')\n",
        "try:\n",
        "  lines = sc.textFile('os_maias.txt')\n",
        "\n",
        "  for line in lines.take(10):\n",
        "    print(line)\n",
        "\n",
        "  sc.stop()\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  sc.stop()"
      ],
      "metadata": {
        "id": "jocH9TZyUbMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Weblog Analysis\n",
        "\n",
        "Consider a set of log files captured during a DDOS (*Distributed Denial of Service*) attack, containing information for the web accesses performed during the attack to the server.\n",
        "\n",
        "The log files contain text lines as shown below, with TAB as the separator:\n",
        "\n",
        "date |IP_source | status_code | operation | URL | execution time |\n",
        "-|-|-|-|-|-\n",
        "timestamp  | string | int | string | string| float |\n",
        "2016-12-06T08:58:35.318+0000|37.139.9.11|404|GET|/codemove/TTCENCUFMH3C|0.026"
      ],
      "metadata": {
        "id": "rsJZWYlHZDJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download the dataset\n",
        "!wget -q -O web.log https://www.dropbox.com/s/0r8902uj9yum7dg/web.log?dl=0\n",
        "!wc web.log"
      ],
      "metadata": {
        "id": "WCWKj68qCOdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1. Count the number of unique IP addresses involved in the DDOS attack.\n"
      ],
      "metadata": {
        "id": "N1-ojIAqCftf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "\n",
        "sc = pyspark.SparkContext('local[*]')\n",
        "try:\n",
        "  lines = sc.textFile('web.log')\n",
        "\n",
        "  for line in lines.take(10):\n",
        "    print(line)\n",
        "\n",
        "  sc.stop()\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  sc.stop()"
      ],
      "metadata": {
        "id": "Y7XoyNETChb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2. For each interval of 10 seconds, provide the following information: [number of requests, average execution time, maximum time, minimum time]"
      ],
      "metadata": {
        "id": "ZJ5TzPdACgQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "\n",
        "sc = pyspark.SparkContext('local[*]')\n",
        "try:\n",
        "  lines = sc.textFile('web.log')\n",
        "\n",
        "  for line in lines.take(10):\n",
        "    print(line)\n",
        "\n",
        "  sc.stop()\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  sc.stop()"
      ],
      "metadata": {
        "id": "M8UVCwcdCwTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3. Create an inverted index that, for each interval of 10 seconds, has a list of (unique) IPs executing accesses (to each URL)."
      ],
      "metadata": {
        "id": "jUHmctaICgtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "\n",
        "sc = pyspark.SparkContext('local[*]')\n",
        "try:\n",
        "  lines = sc.textFile('web.log')\n",
        "\n",
        "  for line in lines.take(10):\n",
        "    print(line)\n",
        "\n",
        "  sc.stop()\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  sc.stop()"
      ],
      "metadata": {
        "id": "RpXghha0C0jC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}