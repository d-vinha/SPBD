{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-vinha/SPBD/blob/main/lab5/SPBD_Labs_spark3_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtsPM1Z4HH7M"
      },
      "source": [
        "# Python Spark SQL Exercises\n",
        "\n",
        "For this set of exercises, you should use SQL statements, as\n",
        "much as possible!\n",
        "\n",
        "Check this online resource for some help with [SQL queries](https://www.codecademy.com/learn/learn-sql/modules/learn-sql-queries/cheatsheet)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Pyspark\n",
        "!pip install --quiet pyspark"
      ],
      "metadata": {
        "id": "BuFS4wO2B1vr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d29648e6-6db5-456d-ac3b-c47282ade10f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download \"Os Maias\"\n",
        "!wget -q -O os_maias.txt https://www.dropbox.com/s/n24v0z7y79np319/os_maias.txt?dl=0\n",
        "!wc os_maias.txt"
      ],
      "metadata": {
        "id": "GEn0_HxQHDlx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d35ed7cd-605a-483a-82bc-ff49c61c0529"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   5877  216896 1292368 os_maias.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Sorted Word Frequency\n",
        "\n",
        "1.1) Create a [Spark SQL](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html) program that counts the number of occurrences of each word in \"Os Maias\" novel, sorting them by frequency (the words with higher occurrence first).\n"
      ],
      "metadata": {
        "id": "f7oXYnylGyko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try :\n",
        "  lines = sc.textFile('os_maias.txt') \\\n",
        "  .filter( lambda line : len(line) > 1 ) \\\n",
        "  .map( lambda line : Row( line = line ) )\n",
        "\n",
        "  linesDF = spark.createDataFrame( lines )\n",
        "  linesDF.createOrReplaceTempView(\"OSMAIAS\")\n",
        "\n",
        "  x = spark.sql(\"SELECT word, count(*) as freq FROM \\\n",
        "                  (SELECT explode(split(line, ' ')) as word FROM OSMAIAS) \\\n",
        "                  GROUP BY word ORDER BY freq DESC\")\n",
        "\n",
        "  x.show(20)\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "  sc.stop()"
      ],
      "metadata": {
        "id": "qMFaHhpWHxkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2) Create a Spark Dataframes program that computes the top 10 most used words in \"Os Maias\" novel."
      ],
      "metadata": {
        "id": "UkI4QSo8Ua35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try :\n",
        "  lines = sc.textFile('os_maias.txt') \\\n",
        "  .filter( lambda line : len(line) > 1 ) \\\n",
        "  .map( lambda line : Row( line = line ) )\n",
        "\n",
        "  linesDF = spark.createDataFrame( lines )\n",
        "  linesDF.createOrReplaceTempView(\"OSMAIAS\")\n",
        "\n",
        "  x = spark.sql(\"SELECT word, count(*) as freq FROM \\\n",
        "                  (SELECT explode(split(line, ' ')) as word FROM OSMAIAS) \\\n",
        "                  GROUP BY word ORDER BY freq DESC LIMIT 10\")\n",
        "\n",
        "  x.show(20)\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "  sc.stop()"
      ],
      "metadata": {
        "id": "jocH9TZyUbMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Weblog Analysis\n",
        "\n",
        "Consider a set of log files captured during a DDOS (*Distributed Denial of Service*) attack, containing information for the web accesses performed during the attack to the server.\n",
        "\n",
        "The log files contain text lines as shown below, with TAB as the separator:\n",
        "\n",
        "date |IP_source | status_code | operation | URL | execution time |\n",
        "-|-|-|-|-|-\n",
        "timestamp  | string | int | string | string| float |\n",
        "2016-12-06T08:58:35.318+0000|37.139.9.11|404|GET|/codemove/TTCENCUFMH3C|0.026"
      ],
      "metadata": {
        "id": "rsJZWYlHZDJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download the dataset\n",
        "!wget -q -O web.log https://www.dropbox.com/s/0r8902uj9yum7dg/web.log?dl=0\n",
        "!wc web.log\n",
        "!head -1 web.log"
      ],
      "metadata": {
        "id": "WCWKj68qCOdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67485b55-8dbb-4bf2-c5fe-9513ac6f0c41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  143457   860742 11758533 web.log\n",
            "2016-12-06T08:58:35.318+0000 37.139.9.11 404 GET /codemove/TTCENCUFMH3C 0.026  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1. Count the number of unique IP addresses involved in the DDOS attack.\n"
      ],
      "metadata": {
        "id": "N1-ojIAqCftf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "\t\t\t\t\t\t.appName('weblog').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "try :\n",
        "    lines = sc.textFile('web.log')\n",
        "    logRows = lines.filter( lambda line : len(line) > 0 ) \\\n",
        "                   .map( lambda line : line.split(' ') ) \\\n",
        "                   .map( lambda l : Row( date = l[0][0:18], \\\n",
        "\t\t\t\t    \t\t            ipSource = l[1], retValue = l[2], \\\n",
        "                            op = l[3], url = l[4], time = float(l[5])))\n",
        "\n",
        "    logRowsDF = spark.createDataFrame( logRows )\n",
        "    logRowsDF.createOrReplaceTempView(\"WEBLOG\")\n",
        "\n",
        "    x = spark.sql(\"SELECT count(*) FROM \\\n",
        "                    (SELECT DISTINCT ipSource FROM WEBLOG)\")\n",
        "\n",
        "    x.show()\n",
        "    sc.stop()\n",
        "except Exception as err:\n",
        "    print(err)\n",
        "    sc.stop()"
      ],
      "metadata": {
        "id": "Y7XoyNETChb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2. For each interval of 10 seconds, provide the following information: [number of requests, average execution time, maximum time, minimum time]"
      ],
      "metadata": {
        "id": "ZJ5TzPdACgQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "\t\t\t\t\t\t.appName('weblog').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "try :\n",
        "    lines = sc.textFile('web.log')\n",
        "    logRows = lines.filter( lambda line : len(line) > 0 ) \\\n",
        "                   .map( lambda line : line.split(' ') ) \\\n",
        "                   .map( lambda l : Row( date = l[0], \\\n",
        "\t\t\t\t    \t\t            ipSource = l[1], retValue = l[2], \\\n",
        "                            op = l[3], url = l[4], time = float(l[5])))\n",
        "\n",
        "    logRowsDF = spark.createDataFrame( logRows )\n",
        "    logRowsDF.createOrReplaceTempView(\"WEBLOG\")\n",
        "\n",
        "    spark.udf.register(\"toInterval\", lambda x : x[0:18])\n",
        "\n",
        "    x = spark.sql(\"SELECT toInterval(date) as intervalo, count(*) as requests, \\\n",
        "                      min(time), max(time), mean(time) FROM WEBLOG \\\n",
        "                      GROUP BY intervalo \\\n",
        "                      ORDER BY requests DESC\")\n",
        "\n",
        "    x.show()\n",
        "    sc.stop()\n",
        "except Exception as err:\n",
        "    print(err)\n",
        "    sc.stop()"
      ],
      "metadata": {
        "id": "M8UVCwcdCwTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3. Create an inverted index that, for each interval of 10 seconds, has a list of (unique) IPs executing accesses (to each URL)."
      ],
      "metadata": {
        "id": "jUHmctaICgtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "\t\t\t\t\t\t.appName('weblog').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "try :\n",
        "    lines = sc.textFile('web.log')\n",
        "    logRows = lines.filter( lambda line : len(line) > 0 ) \\\n",
        "                   .map( lambda line : line.split(' ') ) \\\n",
        "                   .map( lambda l : Row( date = l[0][0:18], \\\n",
        "\t\t\t\t    \t\t            ipSource = l[1], retValue = l[2], \\\n",
        "                            op = l[3], url = l[4], time = float(l[5])))\n",
        "\n",
        "    logRowsDF = spark.createDataFrame( logRows )\n",
        "    logRowsDF.createOrReplaceTempView(\"WEBLOG\")\n",
        "\n",
        "    spark.udf.register(\"toInterval\", lambda x : x[0:18])\n",
        "\n",
        "    x = spark.sql(\"SELECT toInterval(date) as intervalo, collect_set( ipSource) as Ips FROM WEBLOG \\\n",
        "                          GROUP BY intervalo \\\n",
        "                          ORDER BY intervalo DESC\")\n",
        "\n",
        "    x.show()\n",
        "    sc.stop()\n",
        "except Exception as err:\n",
        "    print(err)\n",
        "    sc.stop()"
      ],
      "metadata": {
        "id": "RpXghha0C0jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.4. Create an inverted index that, for each interval of 15 seconds, has a list of (unique) IPs executing accesses (to each URL)."
      ],
      "metadata": {
        "id": "T6W21dG8R2MX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "\t\t\t\t\t\t.appName('weblog').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "try :\n",
        "    lines = sc.textFile('web.log')\n",
        "    logRows = lines.filter( lambda line : len(line) > 0 ) \\\n",
        "                   .map( lambda line : line.split(' ') ) \\\n",
        "                   .map( lambda l : Row( date = l[0], \\\n",
        "\t\t\t\t    \t\t            ipSource = l[1], retValue = l[2], \\\n",
        "                            op = l[3], url = l[4], time = float(l[5])))\n",
        "\n",
        "    logRowsDF = spark.createDataFrame( logRows ).withColumn('date', col('date').cast(\"timestamp\"))\n",
        "    logRowsDF.createOrReplaceTempView(\"WEBLOG\")\n",
        "\n",
        "    spark.udf.register(\"toInterval\", lambda x : x[0:18])\n",
        "\n",
        "    x = spark.sql(\"SELECT from_unixtime((unix_timestamp(date) div 15) * 15) as intervalo, collect_set(ipSource) as Ips \\\n",
        "                      FROM WEBLOG GROUP BY intervalo ORDER BY intervalo\")\n",
        "\n",
        "    x.show()\n",
        "    sc.stop()\n",
        "except Exception as err:\n",
        "    print(err)\n",
        "    sc.stop()"
      ],
      "metadata": {
        "id": "f-OjejqGR8W9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8cc0f93-445f-4c96-f9ac-37903162b641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+--------------------+\n",
            "|          intervalo|                 Ips|\n",
            "+-------------------+--------------------+\n",
            "|2016-12-06 08:58:30|[202.106.16.36, 2...|\n",
            "|2016-12-06 08:58:45|[2a01:488:66:1000...|\n",
            "|2016-12-06 08:59:00|[2a01:488:66:1000...|\n",
            "|2016-12-06 08:59:15|[2a01:488:66:1000...|\n",
            "|2016-12-06 08:59:30|[2a01:488:66:1000...|\n",
            "|2016-12-06 08:59:45|[2a01:488:66:1000...|\n",
            "|2016-12-06 09:00:00|[2a01:488:66:1000...|\n",
            "|2016-12-06 09:00:15|[2a01:488:66:1000...|\n",
            "|2016-12-06 09:00:30|[2a01:488:66:1000...|\n",
            "|2016-12-06 09:00:45|[2a01:488:66:1000...|\n",
            "|2016-12-06 09:01:00|[187.60.170.22, 1...|\n",
            "|2016-12-06 09:01:15|[2001:41d0:8:11c6...|\n",
            "|2016-12-06 09:01:30|[114.215.150.13, ...|\n",
            "|2016-12-06 09:01:45|[103.18.4.13, 120...|\n",
            "|2016-12-06 09:02:00|[120.55.83.30, 18...|\n",
            "|2016-12-06 09:02:15|[185.15.43.51, 20...|\n",
            "|2016-12-06 09:02:30|[120.52.73.97, 17...|\n",
            "|2016-12-06 09:02:45|[177.54.250.18, 1...|\n",
            "|2016-12-06 09:03:15|      [185.15.42.51]|\n",
            "|2016-12-06 09:03:30|      [185.15.42.51]|\n",
            "+-------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}