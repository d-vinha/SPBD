{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-vinha/SPBD/blob/main/lab2/SPBD_Labs_mapreduce2_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtsPM1Z4HH7M"
      },
      "source": [
        "# Python MapReduce Exercises\n",
        "\n",
        "##1. MrJob MapReduce Word Frequency\n",
        "\n",
        "Using the [MrJob](https://mrjob.readthedocs.io/en/latest/) library, create a map-reduce program that counts the number of occurrences of each word, **sorting** them by frequency (the words with higher occurrence first).\n",
        "\n",
        "Check the MrJob documentation to see how multi-step MapReduce jobs can\n",
        "be implemented in the same Python class.\n",
        "\n",
        "Note that you will need a notebook cell to install MrJob before you run your solution, as shown in this week's example notebook."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download the dataset and install MrJob\n",
        "!wget -q -O os_maias.txt https://www.dropbox.com/s/n24v0z7y79np319/os_maias.txt?dl=0\n",
        "!pip install mrjob --quiet\n",
        "!wget -q -O /etc/mrjob.conf https://raw.githubusercontent.com/smduarte/spbd-2324/main/lab2/mrjob.conf"
      ],
      "metadata": {
        "id": "hj6xCxrbXwaU",
        "outputId": "fe84ddd0-48ed-4f30-d682-9d8e2c78dfd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/439.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/439.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m430.1/439.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.6/439.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MrJob Word Frequency 1st Mapper-Reducer\n",
        "Read the words from input and count each ***unique*** word.\n",
        "\n",
        "The processing is split into:\n",
        "\n",
        "+ The mapper emits for each line the number of words\n",
        "+ The reducer sums all the tuples produced by the mapper stage and emit a list of tuples that each unique word and its count\n",
        "\n",
        "Using MrJob, a MapReduce job can be expressed in a single Python class,\n",
        "with methods for each of the phases. The reducer phase is called separately for each key, with the collection of values to be reduced.\n",
        "\n"
      ],
      "metadata": {
        "id": "I3_LlysRX_Mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file eachwordcount.py\n",
        "\n",
        "from mrjob.job import MRJob, MRStep\n",
        "import string\n",
        "\n",
        "max_freq=100000\n",
        "\n",
        "class MRWordCountFrequency(MRJob):\n",
        "\n",
        "    def mapper_words(self, _, line):\n",
        "      line = line.strip()\n",
        "      line = line.translate(str.maketrans('', '', string.punctuation+'«»'))\n",
        "      for word in line.split():\n",
        "        yield word, 1\n",
        "      # get the raw results\n",
        "\n",
        "    def reducer_words(self, key, values):\n",
        "        yield key, sum(values)\n",
        "      # generates tuples with words and the sum of their frequencies\n",
        "\n",
        "    def mapper_partition_sort(self, word, freq):\n",
        "      yield '%05d' % (max_freq-freq), word\n",
        "      # to solve the problem that 11 < 2 and other (because MR uses alphabetical\n",
        "      # sorting of strings) we insert leading 00s; to solve the problem of this\n",
        "      # order being descending, we invert the frequencies of each word by subtra\n",
        "      # it to a high number\n",
        "\n",
        "    def reducer_partition_sort(self, freq, words):\n",
        "      for word in words:\n",
        "        yield word, max_freq-int(freq)\n",
        "      # reverse the tuple again and with the actual frequency\n",
        "      # this will be executed in a partition fashion across multiple machines\n",
        "      # this reducer guarantees the partitions are sorted but there is no final/\n",
        "      # general order\n",
        "\n",
        "    def mapper_total_sort(self, word, freq):\n",
        "      yield None, (word, freq)\n",
        "      # to solve the previous and get the final order - funnel every result to a\n",
        "      # single key (None in this case - a constant in Python for when the key\n",
        "      # does not matter)\n",
        "\n",
        "    def reducer_total_sort(self, _, values):\n",
        "      for word, freq in sorted(values, key= lambda x: x[1], reverse=True):\n",
        "        yield word, freq\n",
        "      # This is only possible because since there is only 1 key (None), this\n",
        "      # will be done in a single machine\n",
        "\n",
        "    def steps(self):\n",
        "        return [ MRStep(mapper=self.mapper_words, reducer=self.reducer_words),\n",
        "                 MRStep(mapper=self.mapper_partition_sort,\n",
        "                        reducer=self.reducer_partition_sort),\n",
        "                 MRStep(mapper=self.mapper_total_sort,\n",
        "                        reducer=self.reducer_total_sort)]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    MRWordCountFrequency.run()"
      ],
      "metadata": {
        "id": "VzO6gmG_X-4G",
        "outputId": "ff3f5890-f034-4ad6-e54a-d9923c784146",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting eachwordcount.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf results\n",
        "!python -m eachwordcount  --output-dir results --cleanup NONE os_maias.txt\n",
        "!head results/*"
      ],
      "metadata": {
        "id": "6eUm2mqigBZb",
        "outputId": "177e9c3b-27e6-4967-d413-b72b4daa7f30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using configs in /etc/mrjob.conf\n",
            "Running step 1 of 3...\n",
            "Creating temp directory /tmp/eachwordcount.root.20230928.102216.385278\n",
            "Running step 2 of 3...\n",
            "Running step 3 of 3...\n",
            "job output is in results\n",
            "\"de\"\t8311\n",
            "\"a\"\t6736\n",
            "\"o\"\t6615\n",
            "\"que\"\t4986\n",
            "\"e\"\t4533\n",
            "\"um\"\t3026\n",
            "\"com\"\t2794\n",
            "\"do\"\t2571\n",
            "\"da\"\t2202\n",
            "\"uma\"\t2170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Weblog Analysis\n",
        "\n",
        "Consider a set of log files captured during a DDOS (*Distributed Denial of Service*) attack, containing information for the web accesses performed during the attack to the server.\n",
        "\n",
        "Create a new notebook that processes the log of web entries using MrJob and map-reduce to:\n",
        "\n",
        "1. Count the number of unique IP addresses involved in the DDOS attack.\n",
        "\n",
        "2. For each interval of 10 seconds, provide the following information: [number of requests, average execution time, maximum time, minimum time]\n",
        "\n",
        "3. Create an inverted index that, for each interval of 10 seconds, has a list of (unique) IPs executing accesses (to each URL).\n",
        "\n",
        "\n",
        "The log files contain text lines as shown below, with TAB as the separator:\n",
        "\n",
        "date |IP_source | status_code | operation | URL | execution time |\n",
        "-|-|-|-|-|-\n",
        "timestamp  | string | int | string | string| float |\n",
        "2016-12-06T08:58:35.318+0000|37.139.9.11|404|GET|/codemove/TTCENCUFMH3C|0.026\n",
        "\n",
        "<br>\n",
        "The log can be downloaded from:\n",
        "\n",
        "[https://www.dropbox.com/s/0r8902uj9yum7dg/web.log?dl=0](https://www.dropbox.com/s/0r8902uj9yum7dg/web.log?dl=0)\n",
        "\n",
        "Suggestion: to start, make a copy an existing notebook and modify it.\n",
        "\n",
        "If you really must..., you can use [dateutil.parser](https://dateutil.readthedocs.io/en/stable/parser.html) for decoding timestamps."
      ],
      "metadata": {
        "id": "rsJZWYlHZDJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O weblog https://www.dropbox.com/s/0r8902uj9yum7dg/web.log?dl=0\n",
        "!wc weblog"
      ],
      "metadata": {
        "id": "JbMuvrZxT9IK",
        "outputId": "aac43a14-7b24-49e1-d5a5-33e3601bb3bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  143457   860742 11758533 weblog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Count the number of unique IP addresses involved in the DDOS attack."
      ],
      "metadata": {
        "id": "NZBFuHiCUI3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file unique_ips.py\n",
        "\n",
        "from mrjob.job import MRJob, MRStep\n",
        "\n",
        "class MRUniqueIPs(MRJob):\n",
        "\n",
        "    def mapper_ip(self, _, line):\n",
        "      _, ip_source, _ = line.strip().split(' ', 2)\n",
        "      yield ip_source, None\n",
        "      # We output the ip_source as the key (value is unimportant, so it is none)\n",
        "\n",
        "    def reducer_ip(self, ip_source, _):\n",
        "      yield None, 1\n",
        "      # the reducer, for each key (ip_source) it is called only once with all\n",
        "      # its repetition\n",
        "      # because we are not interested in the IP itself but just the number of IP\n",
        "      # we use none as the key and 1 as the value to indicate unique keys\n",
        "      # (ip_sources in this case)\n",
        "\n",
        "    def reducer_filter(self, _, ips):\n",
        "      yield \"Unique IPs:\", sum(ips)\n",
        "    # This is only possible because since there is only 1 key (None), this\n",
        "    # will be done in a single machine\n",
        "\n",
        "    def steps(self):\n",
        "      return [MRStep(mapper=self.mapper_ip, reducer=self.reducer_ip),\n",
        "              MRStep(reducer=self.reducer_filter)]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    MRUniqueIPs.run()"
      ],
      "metadata": {
        "id": "4wc3aDQGUMma",
        "outputId": "bb94c8f9-4eb0-495d-a8bf-b568c2f225c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing unique_ips.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf results\n",
        "!python -m unique_ips  --output-dir results --cleanup NONE weblog\n",
        "!head results/*"
      ],
      "metadata": {
        "id": "trtut_JaUZx2",
        "outputId": "8509f13a-b19f-4a97-8570-26a3cb990ca2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using configs in /etc/mrjob.conf\n",
            "Running step 1 of 2...\n",
            "Creating temp directory /tmp/unique_ips.root.20230928.103153.759604\n",
            "Running step 2 of 2...\n",
            "job output is in results\n",
            "\"Unique IPs:\"\t167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. For each 10 sec interval, provide: [num requests, average exec time, max time, min time]"
      ],
      "metadata": {
        "id": "nK0zhMaOV7HE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file interval_info.py\n",
        "\n",
        "from statistics import *\n",
        "from mrjob.job import MRJob, MRStep\n",
        "\n",
        "class MRIntervalInfo(MRJob):\n",
        "\n",
        "  def mapper(self, _, line):\n",
        "        vals = line.strip().split(' ')\n",
        "        timestamp = vals[0]\n",
        "        #getting the date and time\n",
        "        execution_time = float(vals[5])\n",
        "        interval = timestamp[0:18] # YYYY-MM-DDTHH:MM:S -> 10s intervals\n",
        "        #We use the string directely, truncated at the 10s mark (so ignoring the\n",
        "        #last digit of secs)\n",
        "        yield interval, execution_time\n",
        "\n",
        "  def reducer(self, interval, values):\n",
        "      times = list(values)\n",
        "      # we funnel results of values to a list, transforming the generator\n",
        "      # (values) because as a list it can be used to compute the times\n",
        "      yield interval, (len(times), mean(times), max(times), min(times))\n",
        "\n",
        "  def steps(self):\n",
        "      return [MRStep(mapper=self.mapper, reducer=self.reducer)]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    MRIntervalInfo.run()"
      ],
      "metadata": {
        "id": "wUYXQ5goWDLO",
        "outputId": "c031cebd-87d3-44ab-af20-1c29b8ba1922",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting interval_info.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf results\n",
        "!python -m interval_info --output-dir results --cleanup NONE weblog\n",
        "!head results/*"
      ],
      "metadata": {
        "id": "K5H9SaxcWDe5",
        "outputId": "b0c6a090-ace7-4a27-c405-f78d5722a426",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using configs in /etc/mrjob.conf\n",
            "Running step 1 of 1...\n",
            "Creating temp directory /tmp/interval_info.root.20230928.103806.524450\n",
            "job output is in results\n",
            "==> results/part-00000 <==\n",
            "\"2016-12-06T08:58:3\"\t[483, 7.593424430641822, 46.849, 0.013]\n",
            "\"2016-12-06T08:58:4\"\t[2611, 30.15984565300651, 69.654, 0.014]\n",
            "\"2016-12-06T08:58:5\"\t[5500, 38.52511163636364, 80.846, 0.017]\n",
            "\"2016-12-06T08:59:0\"\t[6914, 38.534382123228234, 81.659, 0.018]\n",
            "\"2016-12-06T08:59:1\"\t[6271, 32.96384978472333, 83.993, 0.017]\n",
            "\"2016-12-06T08:59:2\"\t[5434, 17.29333143172617, 77.967, 0.051]\n",
            "\"2016-12-06T08:59:3\"\t[8015, 11.21015221459763, 67.441, 0.056]\n",
            "\"2016-12-06T08:59:4\"\t[7947, 7.7618157795394485, 65.706, 0.914]\n",
            "\n",
            "==> results/part-00001 <==\n",
            "\"2016-12-06T08:59:5\"\t[5983, 3.8216643824168477, 54.29, 0.678]\n",
            "\"2016-12-06T09:00:0\"\t[6882, 8.649971519907004, 45.314, 0.017]\n",
            "\"2016-12-06T09:00:1\"\t[9719, 7.857372672085606, 34.406, 0.225]\n",
            "\"2016-12-06T09:00:2\"\t[6616, 4.611345223700121, 25.847, 0.014]\n",
            "\"2016-12-06T09:00:3\"\t[6771, 1.6047638458130262, 26.53, 0.007]\n",
            "\n",
            "==> results/part-00002 <==\n",
            "\"2016-12-06T09:00:4\"\t[12135, 0.32576440049443756, 9.537, 0.007]\n",
            "\"2016-12-06T09:00:5\"\t[8062, 0.20009092036715456, 1.905, 0.006]\n",
            "\"2016-12-06T09:01:0\"\t[8747, 0.17546667428832743, 1.624, 0.005]\n",
            "\"2016-12-06T09:01:1\"\t[10552, 0.1480401819560273, 1.506, 0.004]\n",
            "\n",
            "==> results/part-00003 <==\n",
            "\"2016-12-06T09:01:2\"\t[5315, 0.15367055503292568, 1.361, 0.005]\n",
            "\"2016-12-06T09:01:3\"\t[6163, 0.11656384877494727, 1.117, 0.005]\n",
            "\"2016-12-06T09:01:4\"\t[5294, 0.10019361541367586, 1.339, 0.005]\n",
            "\"2016-12-06T09:01:5\"\t[3343, 0.09841130720909362, 1.098, 0.005]\n",
            "\"2016-12-06T09:02:0\"\t[1950, 0.0921528205128205, 0.972, 0.005]\n",
            "\"2016-12-06T09:02:1\"\t[1006, 0.047665009940357855, 0.821, 0.005]\n",
            "\"2016-12-06T09:02:2\"\t[112, 0.03477678571428571, 0.812, 0.006]\n",
            "\"2016-12-06T09:02:3\"\t[15, 0.0754, 0.153, 0.008]\n",
            "\"2016-12-06T09:02:4\"\t[6, 0.041, 0.064, 0.011]\n",
            "\"2016-12-06T09:02:5\"\t[1, 0.059, 0.059, 0.059]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Create an inverted index that, for each interval of 10 seconds, has a list of (unique) IPs executing accesses (to each URL)."
      ],
      "metadata": {
        "id": "Hx1MMTHRXjeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file inverted_index.py\n",
        "\n",
        "from mrjob.job import MRJob, MRStep\n",
        "\n",
        "class MRInvertedIndex(MRJob):\n",
        "\n",
        "  def mapper(self, _, line):\n",
        "        vals = line.strip().split(' ')\n",
        "        if len(vals) >= 6:\n",
        "          timestamp = vals[0]\n",
        "          interval = timestamp[0:18] # YYYY-MM-DDTHH:MM:S -> 10s intervals\n",
        "\n",
        "          source_ip = vals[1]\n",
        "          target_url = vals[4]\n",
        "          yield \"{}-{}\".format(interval, target_url), source_ip\n",
        "          # my key is\n",
        "\n",
        "  def reducer(self, key, values):\n",
        "    yield key, set(values)\n",
        "    # instead of a list, to remove repetitions, we use a set\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    MRInvertedIndex.run()"
      ],
      "metadata": {
        "id": "e1adGFSvXl6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf results\n",
        "!python -m inverted_index --output-dir results web.log && head results/*"
      ],
      "metadata": {
        "id": "8am2OeIqXzAZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}