{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-vinha/SPBD/blob/main/lab1/SPBD_Labs_mapreduce1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKGXsDUIDxH6"
      },
      "source": [
        "# Python MapReduce Example\n",
        "\n",
        "Word count implemented in pure Python.\n",
        "\n",
        "This notebook exemplifies the execution of a map-reduce program in Python, using Hadoop.\n",
        "In this example, hadoop runs in standalone mode and reads data from the local filesystem, while in cluster mode data is read typically from HDFS dsitributed file system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBR4IYlXDxH9"
      },
      "source": [
        "### Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "scrolled": false,
        "id": "B68ZyGdPDxH9"
      },
      "outputs": [],
      "source": [
        "!wget -q -O os_maias.txt https://www.dropbox.com/s/n24v0z7y79np319/os_maias.txt?dl=0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oNh82tSnNC-K",
        "outputId": "0cc00e26-68c9-40e5-e406-b9947c99bc96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8sllMoIDxH_"
      },
      "source": [
        "## WordCount Example\n",
        "Read the words from input and count them.\n",
        "\n",
        "The processing is split into two steps:\n",
        "\n",
        "+ The mapper emits for each line the number of words\n",
        "+ The reduces sums all the tuples produced by the mapper stage..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQd9aMLeDxH_"
      },
      "source": [
        "### Mapper\n",
        "\n",
        "By starting an element with \"%%file\", you are specifying that when run, the contents are written to the local disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LbItx5zwDxIA",
        "outputId": "4a5488c0-94aa-4b81-d37c-696c9cd98952",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper.py\n"
          ]
        }
      ],
      "source": [
        "%%file mapper.py\n",
        "#!/usr/bin/env python\n",
        "\n",
        "# import sys\n",
        "import sys\n",
        "# import string library function\n",
        "import string\n",
        "\n",
        "# input comes from STDIN (standard input)\n",
        "for line in sys.stdin:\n",
        "    # remove leading and trailing whitespace\n",
        "    line = line.strip()\n",
        "    # remove punctuation characters\n",
        "    line = line.translate(str.maketrans('', '', string.punctuation+'«»'))\n",
        "    # split the line into words\n",
        "    words = line.split()\n",
        "    print('words\\t%s' % len(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f1-nQlcDxIA"
      },
      "source": [
        "### Reducer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qCDCm5cnDxIB",
        "outputId": "a01808c6-4a9d-49f9-da64-9be27523349c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting reducer.py\n"
          ]
        }
      ],
      "source": [
        "%%file reducer.py\n",
        "#!/usr/bin/env python\n",
        "\n",
        "import sys\n",
        "\n",
        "total_count = 0\n",
        "\n",
        "# input comes from STDIN\n",
        "for line in sys.stdin:\n",
        "    # remove leading and trailing whitespace\n",
        "    line = line.strip()\n",
        "\n",
        "    # parse the input we got from mapper.py\n",
        "    key, count = line.split('\\t', 1)\n",
        "\n",
        "    # convert count (currently a string) to int\n",
        "    count = int(count)\n",
        "\n",
        "    total_count += count\n",
        "\n",
        "print('words\\t%s' % (total_count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stOA47uRDxIC"
      },
      "source": [
        "## Local execution\n",
        "\n",
        "The scripts can be tested using just the unix shell, as follows..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cORkS8pvDxIC"
      },
      "source": [
        "### Make the scripts executable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NuBS8D79DxID"
      },
      "outputs": [],
      "source": [
        "!chmod a+x mapper.py && chmod a+x reducer.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dZTi7eIDxID"
      },
      "source": [
        "### Execute\n",
        "\n",
        "The execution workflow is as follows:\n",
        "\n",
        "+ The input file is piped into the input of the mapper;\n",
        "+ The output the mapper is sorted;\n",
        "+ The sorted output of the mapper is fed to the reducer stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PwZMaB-hDxIE",
        "outputId": "13fc7c3c-c816-4042-dafe-84a6eb557873",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "words\t213359\n"
          ]
        }
      ],
      "source": [
        "!cat \"os_maias.txt\" | ./mapper.py | sort -k1,1 | ./reducer.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MapReduce with HADOOP"
      ],
      "metadata": {
        "id": "rwuFmaseENu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Hadoop on Google Colab\n",
        "!curl -s https://raw.githubusercontent.com/smduarte/spbd-2324/main/lab1/install_hadoop.sh | bash"
      ],
      "metadata": {
        "id": "GnLDSPxo4gKk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC2bKZHtDxIE"
      },
      "source": [
        "## Hadoop standalone mode execution\n",
        "\n",
        "For executing in an hadoop cluster, input data should be moved into an HDFS directory. For executing in standalone mode, data can be read from the local filesystem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z1JJmqvDxIE"
      },
      "source": [
        "\n",
        "The output directory needs to be cleared..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RCwnGGz7DxIF"
      },
      "outputs": [],
      "source": [
        "!rm -rf results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xf3e1UGDxIF"
      },
      "source": [
        "### Submitting the job\n",
        "\n",
        "The _hadoop_ command is used to submit the mapreduce job to the cluster..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sTxBwSF8DxIG",
        "outputId": "b68c4216-5259-4a24-f456-8a5fb59de024",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-14 10:41:21,233 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2023-09-14 10:41:21,461 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2023-09-14 10:41:21,461 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2023-09-14 10:41:21,489 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2023-09-14 10:41:21,812 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2023-09-14 10:41:21,838 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2023-09-14 10:41:22,277 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local738239175_0001\n",
            "2023-09-14 10:41:22,277 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2023-09-14 10:41:22,665 INFO mapred.LocalDistributedCacheManager: Localized file:/content/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local738239175_0001_224ccbb8-fe54-4797-89c1-60e4215cf220/mapper.py\n",
            "2023-09-14 10:41:22,691 INFO mapred.LocalDistributedCacheManager: Localized file:/content/reducer.py as file:/tmp/hadoop-root/mapred/local/job_local738239175_0001_be56b678-5c3d-4239-8d64-bec8a9be8f0d/reducer.py\n",
            "2023-09-14 10:41:22,790 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2023-09-14 10:41:22,792 INFO mapreduce.Job: Running job: job_local738239175_0001\n",
            "2023-09-14 10:41:22,799 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2023-09-14 10:41:22,802 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2023-09-14 10:41:22,815 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-09-14 10:41:22,815 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-09-14 10:41:22,897 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2023-09-14 10:41:22,902 INFO mapred.LocalJobRunner: Starting task: attempt_local738239175_0001_m_000000_0\n",
            "2023-09-14 10:41:22,956 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-09-14 10:41:22,958 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-09-14 10:41:23,001 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-09-14 10:41:23,017 INFO mapred.MapTask: Processing split: file:/content/os_maias.txt:0+1292368\n",
            "2023-09-14 10:41:23,048 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2023-09-14 10:41:23,214 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2023-09-14 10:41:23,214 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2023-09-14 10:41:23,214 INFO mapred.MapTask: soft limit at 83886080\n",
            "2023-09-14 10:41:23,214 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2023-09-14 10:41:23,214 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2023-09-14 10:41:23,218 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2023-09-14 10:41:23,225 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./mapper.py]\n",
            "2023-09-14 10:41:23,232 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2023-09-14 10:41:23,235 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2023-09-14 10:41:23,235 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2023-09-14 10:41:23,236 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2023-09-14 10:41:23,238 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2023-09-14 10:41:23,238 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2023-09-14 10:41:23,242 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2023-09-14 10:41:23,244 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2023-09-14 10:41:23,244 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2023-09-14 10:41:23,244 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2023-09-14 10:41:23,245 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2023-09-14 10:41:23,246 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2023-09-14 10:41:23,292 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-09-14 10:41:23,292 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-09-14 10:41:23,300 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-09-14 10:41:23,382 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-09-14 10:41:23,418 INFO streaming.PipeMapRed: Records R/W=1749/1\n",
            "2023-09-14 10:41:23,628 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2023-09-14 10:41:23,629 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2023-09-14 10:41:23,634 INFO mapred.LocalJobRunner: \n",
            "2023-09-14 10:41:23,635 INFO mapred.MapTask: Starting flush of map output\n",
            "2023-09-14 10:41:23,635 INFO mapred.MapTask: Spilling map output\n",
            "2023-09-14 10:41:23,635 INFO mapred.MapTask: bufstart = 0; bufend = 51885; bufvoid = 104857600\n",
            "2023-09-14 10:41:23,635 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26190892(104763568); length = 23505/6553600\n",
            "2023-09-14 10:41:23,665 INFO mapred.MapTask: Finished spill 0\n",
            "2023-09-14 10:41:23,683 INFO mapred.Task: Task:attempt_local738239175_0001_m_000000_0 is done. And is in the process of committing\n",
            "2023-09-14 10:41:23,687 INFO mapred.LocalJobRunner: Records R/W=1749/1\n",
            "2023-09-14 10:41:23,687 INFO mapred.Task: Task 'attempt_local738239175_0001_m_000000_0' done.\n",
            "2023-09-14 10:41:23,695 INFO mapred.Task: Final Counters for attempt_local738239175_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1434589\n",
            "\t\tFILE: Number of bytes written=847057\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=5877\n",
            "\t\tMap output records=5877\n",
            "\t\tMap output bytes=51885\n",
            "\t\tMap output materialized bytes=63645\n",
            "\t\tInput split bytes=78\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=5877\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=331350016\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1292368\n",
            "2023-09-14 10:41:23,695 INFO mapred.LocalJobRunner: Finishing task: attempt_local738239175_0001_m_000000_0\n",
            "2023-09-14 10:41:23,695 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2023-09-14 10:41:23,701 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2023-09-14 10:41:23,702 INFO mapred.LocalJobRunner: Starting task: attempt_local738239175_0001_r_000000_0\n",
            "2023-09-14 10:41:23,714 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-09-14 10:41:23,714 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-09-14 10:41:23,714 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-09-14 10:41:23,719 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2c9fc4ec\n",
            "2023-09-14 10:41:23,721 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2023-09-14 10:41:23,748 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2023-09-14 10:41:23,751 INFO reduce.EventFetcher: attempt_local738239175_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2023-09-14 10:41:23,798 INFO mapreduce.Job: Job job_local738239175_0001 running in uber mode : false\n",
            "2023-09-14 10:41:23,807 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local738239175_0001_m_000000_0 decomp: 63641 len: 63645 to MEMORY\n",
            "2023-09-14 10:41:23,809 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2023-09-14 10:41:23,817 INFO reduce.InMemoryMapOutput: Read 63641 bytes from map-output for attempt_local738239175_0001_m_000000_0\n",
            "2023-09-14 10:41:23,822 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 63641, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->63641\n",
            "2023-09-14 10:41:23,825 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2023-09-14 10:41:23,827 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2023-09-14 10:41:23,827 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2023-09-14 10:41:23,835 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2023-09-14 10:41:23,835 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 63633 bytes\n",
            "2023-09-14 10:41:23,861 INFO reduce.MergeManagerImpl: Merged 1 segments, 63641 bytes to disk to satisfy reduce memory limit\n",
            "2023-09-14 10:41:23,861 INFO reduce.MergeManagerImpl: Merging 1 files, 63645 bytes from disk\n",
            "2023-09-14 10:41:23,862 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2023-09-14 10:41:23,862 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2023-09-14 10:41:23,863 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 63633 bytes\n",
            "2023-09-14 10:41:23,864 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2023-09-14 10:41:23,872 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./reducer.py]\n",
            "2023-09-14 10:41:23,876 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2023-09-14 10:41:23,882 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2023-09-14 10:41:23,908 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-09-14 10:41:23,908 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-09-14 10:41:23,909 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-09-14 10:41:23,929 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-09-14 10:41:24,036 INFO streaming.PipeMapRed: Records R/W=5877/1\n",
            "2023-09-14 10:41:24,041 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2023-09-14 10:41:24,041 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2023-09-14 10:41:24,043 INFO mapred.Task: Task:attempt_local738239175_0001_r_000000_0 is done. And is in the process of committing\n",
            "2023-09-14 10:41:24,044 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2023-09-14 10:41:24,046 INFO mapred.Task: Task attempt_local738239175_0001_r_000000_0 is allowed to commit now\n",
            "2023-09-14 10:41:24,048 INFO output.FileOutputCommitter: Saved output of task 'attempt_local738239175_0001_r_000000_0' to file:/content/results\n",
            "2023-09-14 10:41:24,053 INFO mapred.LocalJobRunner: Records R/W=5877/1 > reduce\n",
            "2023-09-14 10:41:24,053 INFO mapred.Task: Task 'attempt_local738239175_0001_r_000000_0' done.\n",
            "2023-09-14 10:41:24,056 INFO mapred.Task: Final Counters for attempt_local738239175_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1561911\n",
            "\t\tFILE: Number of bytes written=910727\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=63645\n",
            "\t\tReduce input records=5877\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=5877\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=331350016\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=25\n",
            "2023-09-14 10:41:24,056 INFO mapred.LocalJobRunner: Finishing task: attempt_local738239175_0001_r_000000_0\n",
            "2023-09-14 10:41:24,057 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2023-09-14 10:41:24,810 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2023-09-14 10:41:24,810 INFO mapreduce.Job: Job job_local738239175_0001 completed successfully\n",
            "2023-09-14 10:41:24,825 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=2996500\n",
            "\t\tFILE: Number of bytes written=1757784\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=5877\n",
            "\t\tMap output records=5877\n",
            "\t\tMap output bytes=51885\n",
            "\t\tMap output materialized bytes=63645\n",
            "\t\tInput split bytes=78\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=63645\n",
            "\t\tReduce input records=5877\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=11754\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=662700032\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1292368\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=25\n",
            "2023-09-14 10:41:24,825 INFO streaming.StreamJob: Output directory: results\n"
          ]
        }
      ],
      "source": [
        "!hadoop jar /usr/local/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-*streaming*.jar -files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py -input os_maias.txt -output results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPzJQj5qDxIG"
      },
      "source": [
        "#### Checking the results\n",
        "The result is stored in directory results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0Tow7rp4DxIG",
        "outputId": "eeb2041a-502f-4143-e398-a8ed28f89741",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "words\t213359\n"
          ]
        }
      ],
      "source": [
        "!cat results/part-*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}